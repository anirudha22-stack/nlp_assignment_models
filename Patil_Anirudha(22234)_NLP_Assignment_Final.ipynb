{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "L_jOA4yC48Nn"
      },
      "id": "L_jOA4yC48Nn"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "84b8b960-7711-444c-aa5d-78af50b86006",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84b8b960-7711-444c-aa5d-78af50b86006",
        "outputId": "d6fad47f-62ca-48cb-861d-5a81f5c3f6de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Training Data Info ---\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 41157 entries, 0 to 41156\n",
            "Data columns (total 6 columns):\n",
            " #   Column         Non-Null Count  Dtype \n",
            "---  ------         --------------  ----- \n",
            " 0   UserName       41157 non-null  int64 \n",
            " 1   ScreenName     41157 non-null  int64 \n",
            " 2   Location       32567 non-null  object\n",
            " 3   TweetAt        41157 non-null  object\n",
            " 4   OriginalTweet  41157 non-null  object\n",
            " 5   Sentiment      41157 non-null  object\n",
            "dtypes: int64(2), object(4)\n",
            "memory usage: 1.9+ MB\n",
            "\n",
            "--- Sample Training Data ---\n",
            "   UserName  ScreenName   Location     TweetAt  \\\n",
            "0      3799       48751     London  16-03-2020   \n",
            "1      3800       48752         UK  16-03-2020   \n",
            "2      3801       48753  Vagabonds  16-03-2020   \n",
            "3      3802       48754        NaN  16-03-2020   \n",
            "4      3803       48755        NaN  16-03-2020   \n",
            "\n",
            "                                       OriginalTweet           Sentiment  \n",
            "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...             Neutral  \n",
            "1  advice Talk to your neighbours family to excha...            Positive  \n",
            "2  Coronavirus Australia: Woolworths to give elde...            Positive  \n",
            "3  My food stock is not the only one which is emp...            Positive  \n",
            "4  Me, ready to go at supermarket during the #COV...  Extremely Negative  \n",
            "\n",
            "Training data shape: (41157, 6)\n",
            "Test data shape: (3798, 6)\n",
            "\n",
            "Sentiment distribution in training data:\n",
            "Sentiment\n",
            "Positive              11422\n",
            "Negative               9917\n",
            "Neutral                7713\n",
            "Extremely Positive     6624\n",
            "Extremely Negative     5481\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import pickle # To save vocabulary\n",
        "from collections import Counter\n",
        "\n",
        "# --- Load Data ---\n",
        "# URLs for the raw CSV files on GitHub\n",
        "train_url = 'https://raw.githubusercontent.com/islnlp/Advance-NLP-assingment-/main/Corona_NLP_train.csv'\n",
        "test_url = 'https://raw.githubusercontent.com/islnlp/Advance-NLP-assingment-/main/Corona_NLP_test.csv'\n",
        "\n",
        "# It's good practice to specify encoding\n",
        "df_train = pd.read_csv(train_url, encoding='latin1')\n",
        "df_test = pd.read_csv(test_url, encoding='latin1')\n",
        "\n",
        "print(\"--- Training Data Info ---\")\n",
        "df_train.info()\n",
        "print(\"\\n--- Sample Training Data ---\")\n",
        "print(df_train.head())\n",
        "\n",
        "# --- Basic Exploration ---\n",
        "print(f\"\\nTraining data shape: {df_train.shape}\")\n",
        "print(f\"Test data shape: {df_test.shape}\")\n",
        "print(f\"\\nSentiment distribution in training data:\\n{df_train['Sentiment'].value_counts()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "94b89832-b617-48e1-95f7-55ccd9ae67cd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94b89832-b617-48e1-95f7-55ccd9ae67cd",
        "outputId": "65a10c04-6165-4fb3-8f24-2dc7c3d0d7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Classes: ['Neutral' 'Positive' 'Extremely Negative' 'Negative' 'Extremely Positive']\n"
          ]
        }
      ],
      "source": [
        "# List of unique classes\n",
        "classes = df_train['Sentiment'].unique()\n",
        "print(\"Available Classes:\", classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "209fca77-69a0-4d12-a41e-b5e596d3896e",
      "metadata": {
        "id": "209fca77-69a0-4d12-a41e-b5e596d3896e"
      },
      "source": [
        "## Part 1. Data Preprocessing Step"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3010b981-f7f3-47a9-b897-f202b4bc2f2c",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "3010b981-f7f3-47a9-b897-f202b4bc2f2c"
      },
      "source": [
        "### 1. Text Cleaning and Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "81043cc6-9169-4451-bd36-dabfe3e91917",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81043cc6-9169-4451-bd36-dabfe3e91917",
        "outputId": "f3fc3564-ae3d-4914-a2de-33b83ef5129a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Processed Tokens ---\n",
            "                                       OriginalTweet  \\\n",
            "0  @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   \n",
            "1  advice Talk to your neighbours family to excha...   \n",
            "2  Coronavirus Australia: Woolworths to give elde...   \n",
            "3  My food stock is not the only one which is emp...   \n",
            "4  Me, ready to go at supermarket during the #COV...   \n",
            "\n",
            "                                              tokens  \n",
            "0                                         [and, and]  \n",
            "1  [advice, talk, to, your, neighbours, family, t...  \n",
            "2  [coronavirus, australia, woolworths, to, give,...  \n",
            "3  [my, food, stock, is, not, the, only, one, whi...  \n",
            "4  [me, ready, to, go, at, supermarket, during, t...  \n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Cleans and tokenizes a single text string.\n",
        "    \"\"\"\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Remove Twitter handles\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove non-alphanumeric characters (keep spaces)\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Tokenize by splitting on whitespace\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "# Apply preprocessing to the tweet column\n",
        "df_train['tokens'] = df_train['OriginalTweet'].apply(preprocess_text)\n",
        "df_test['tokens'] = df_test['OriginalTweet'].apply(preprocess_text)\n",
        "\n",
        "print(\"\\n--- Sample Processed Tokens ---\")\n",
        "print(df_train[['OriginalTweet', 'tokens']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "ab1847bd-63f9-428e-b1c9-92432dbda0ee",
      "metadata": {
        "id": "ab1847bd-63f9-428e-b1c9-92432dbda0ee"
      },
      "outputs": [],
      "source": [
        "# Calculate the length of each token list\n",
        "df_train['token_len'] = df_train['tokens'].apply(len)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b5b18cc7-94b7-4684-af96-f877ea01ba1b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5b18cc7-94b7-4684-af96-f877ea01ba1b",
        "outputId": "ac8cf839-9934-4c42-e947-7620d0bb9c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Analysis of Tweet Lengths ---\n",
            "count    41157.000000\n",
            "mean        28.770051\n",
            "std         11.526695\n",
            "min          0.000000\n",
            "50%         30.000000\n",
            "90%         43.000000\n",
            "95%         46.000000\n",
            "98%         49.000000\n",
            "99%         51.000000\n",
            "max         62.000000\n",
            "Name: token_len, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Analyze the distribution of token lengths\n",
        "print(\"\\n--- Analysis of Tweet Lengths ---\")\n",
        "print(df_train['token_len'].describe(percentiles=[0.90, 0.95, 0.98, 0.99]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b90af21-b137-4923-b89d-c652f93d26f9",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "0b90af21-b137-4923-b89d-c652f93d26f9"
      },
      "source": [
        "### 2. Vocabulary Building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "48460559-7b78-431e-b9a1-fe8424f5b6c6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48460559-7b78-431e-b9a1-fe8424f5b6c6",
        "outputId": "e7c0b468-4d1f-4aad-e840-9644acdd815b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Vocabulary size: 10000\n"
          ]
        }
      ],
      "source": [
        "def build_vocab(token_lists, max_vocab_size=10000):\n",
        "    \"\"\"\n",
        "    Builds a vocabulary from a list of token lists.\n",
        "    \"\"\"\n",
        "    # Count word frequencies\n",
        "    word_counts = Counter(word for tokens in token_lists for word in tokens)\n",
        "\n",
        "    # Get the most common words\n",
        "    most_common_words = word_counts.most_common(max_vocab_size - 2) # Reserve space for padding and unknown\n",
        "\n",
        "    # Create word-to-index mapping\n",
        "    # <PAD>: Padding token, index 0\n",
        "    # <UNK>: Unknown word token, index 1\n",
        "    word_to_idx = {'<PAD>': 0, '<UNK>': 1}\n",
        "    for i, (word, _) in enumerate(most_common_words):\n",
        "        word_to_idx[word] = i + 2\n",
        "\n",
        "    return word_to_idx\n",
        "\n",
        "# Build vocabulary from the training data\n",
        "vocab = build_vocab(df_train['tokens'])\n",
        "print(f\"\\nVocabulary size: {len(vocab)}\")\n",
        "\n",
        "# Save the vocabulary for later use\n",
        "with open('vocab.pkl', 'wb') as f:\n",
        "    pickle.dump(vocab, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8239030a-9260-4d35-91dc-d7029394de49",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8239030a-9260-4d35-91dc-d7029394de49"
      },
      "source": [
        "### 3. Text Encoding and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e258c4ac-fe92-4093-ba4d-2cdd1893e652",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e258c4ac-fe92-4093-ba4d-2cdd1893e652",
        "outputId": "16c640d6-7956-47be-acd2-a1b074b30f34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Sample Encoded and Padded Sequence ---\n",
            "Original Tokens: ['and', 'and']\n",
            "Encoded Sequence: [4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ],
      "source": [
        "def encode_and_pad(tokens, word_to_idx, max_len=60):\n",
        "    \"\"\"\n",
        "    Encodes tokens to indices and pads/truncates the sequence.\n",
        "    \"\"\"\n",
        "    # Encode tokens to indices, using <UNK> for out-of-vocab words\n",
        "    encoded = [word_to_idx.get(token, word_to_idx['<UNK>']) for token in tokens]\n",
        "\n",
        "    # Pad or truncate\n",
        "    if len(encoded) < max_len:\n",
        "        # Pad with <PAD> token (index 0)\n",
        "        encoded += [word_to_idx['<PAD>']] * (max_len - len(encoded))\n",
        "    else:\n",
        "        # Truncate\n",
        "        encoded = encoded[:max_len]\n",
        "\n",
        "    return encoded\n",
        "\n",
        "# Define a fixed sequence length\n",
        "MAX_SEQUENCE_LENGTH = 50\n",
        "\n",
        "# Apply encoding and padding\n",
        "df_train['encoded'] = df_train['tokens'].apply(lambda x: encode_and_pad(x, vocab, MAX_SEQUENCE_LENGTH))\n",
        "df_test['encoded'] = df_test['tokens'].apply(lambda x: encode_and_pad(x, vocab, MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "print(\"\\n--- Sample Encoded and Padded Sequence ---\")\n",
        "print(f\"Original Tokens: {df_train['tokens'].iloc[0]}\")\n",
        "print(f\"Encoded Sequence: {df_train['encoded'].iloc[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9f14c57-5552-4bbf-89b4-b63bc7fe9944",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "d9f14c57-5552-4bbf-89b4-b63bc7fe9944"
      },
      "source": [
        "### 4. Label Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7d4e7f99-c23f-4007-a58b-ba981b472138",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d4e7f99-c23f-4007-a58b-ba981b472138",
        "outputId": "bf6c03bf-abb1-4046-fed3-9ee6430a08e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Label Mapping ---\n",
            "{'Extremely Negative': 0, 'Negative': 1, 'Neutral': 2, 'Positive': 3, 'Extremely Positive': 4}\n"
          ]
        }
      ],
      "source": [
        "# Create a mapping from sentiment to integer\n",
        "label_map = {\n",
        "    'Extremely Negative': 0,\n",
        "    'Negative': 1,\n",
        "    'Neutral': 2,\n",
        "    'Positive': 3,\n",
        "    'Extremely Positive': 4\n",
        "}\n",
        "# And the reverse mapping for interpretation later\n",
        "idx_to_label = {v: k for k, v in label_map.items()}\n",
        "\n",
        "df_train['label'] = df_train['Sentiment'].map(label_map)\n",
        "df_test['label'] = df_test['Sentiment'].map(label_map)\n",
        "\n",
        "print(\"\\n--- Label Mapping ---\")\n",
        "print(label_map)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22c91914-5ddd-479e-bfc2-c393ed9d8143",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "22c91914-5ddd-479e-bfc2-c393ed9d8143"
      },
      "source": [
        "### 5. Final Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4f866d29-f612-45e1-ac03-6b988d719a9d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f866d29-f612-45e1-ac03-6b988d719a9d",
        "outputId": "db643e5c-662a-468a-8b03-d3c06d323024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final training data shape: X_train=(41157, 50), y_train=(41157,)\n",
            "Final testing data shape: X_test=(3798, 50), y_test=(3798,)\n"
          ]
        }
      ],
      "source": [
        "# Convert final data into NumPy arrays\n",
        "X_train = np.array(df_train['encoded'].tolist())\n",
        "y_train = np.array(df_train['label'].tolist())\n",
        "\n",
        "X_test = np.array(df_test['encoded'].tolist())\n",
        "y_test = np.array(df_test['label'].tolist())\n",
        "\n",
        "print(f\"\\nFinal training data shape: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
        "print(f\"Final testing data shape: X_test={X_test.shape}, y_test={y_test.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b96a9605-14ad-489e-9864-faf2ea50de78",
      "metadata": {
        "id": "b96a9605-14ad-489e-9864-faf2ea50de78"
      },
      "source": [
        "## Part 2. Building Essentials for Model (Layers, Activationn Optimizers, Loss Function)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e381588-b1d1-47ae-ba5d-d5481b1c71e5",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "8e381588-b1d1-47ae-ba5d-d5481b1c71e5"
      },
      "source": [
        "### 1. Base Layer and Activation Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "c21f3722-6e74-4b00-a483-118b8ab92546",
      "metadata": {
        "id": "c21f3722-6e74-4b00-a483-118b8ab92546"
      },
      "outputs": [],
      "source": [
        "class Layer:\n",
        "    \"\"\"Base class for all layers.\"\"\"\n",
        "    def __init__(self):\n",
        "        self.params = {} # For weights and biases\n",
        "        self.grads = {}  # For gradients\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        raise NotImplementedError\n",
        "\n",
        "    def backward(self, grad):\n",
        "        raise NotImplementedError\n",
        "\n",
        "class Tanh:\n",
        "    \"\"\"Tanh activation function.\"\"\"\n",
        "    def forward(self, x):\n",
        "        self.y = np.tanh(x)\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, grad):\n",
        "        return grad * (1 - self.y**2)\n",
        "\n",
        "class Sigmoid:\n",
        "    \"\"\"Sigmoid activation function.\"\"\"\n",
        "    def forward(self, x):\n",
        "        self.y = 1 / (1 + np.exp(-x))\n",
        "        return self.y\n",
        "\n",
        "    def backward(self, grad):\n",
        "        return grad * self.y * (1 - self.y)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912cd981-1566-4e9b-9092-d77eb8d8af69",
      "metadata": {
        "id": "912cd981-1566-4e9b-9092-d77eb8d8af69"
      },
      "source": [
        "### 2. Core Network Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "044c0b87-3f5f-42e1-a2fc-c24607855a3b",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "044c0b87-3f5f-42e1-a2fc-c24607855a3b"
      },
      "source": [
        "#### Embedding Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "44ff3f71-e9a2-4362-83d5-cafc7b3d19af",
      "metadata": {
        "id": "44ff3f71-e9a2-4362-83d5-cafc7b3d19af"
      },
      "outputs": [],
      "source": [
        "class Embedding(Layer):\n",
        "    \"\"\"\n",
        "    Embedding layer: turns positive integers (indexes) into dense vectors of fixed size.\n",
        "    e.g. [[4], [20]] -> [[0.25, 0.1], [0.6, -0.2]]\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Xavier Glorot initialization\n",
        "        limit = np.sqrt(6 / (vocab_size + embed_dim))\n",
        "        self.params['W'] = np.random.uniform(-limit, limit, (vocab_size, embed_dim))\n",
        "        self.grads['W'] = np.zeros_like(self.params['W'])\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Inputs shape: (batch_size, seq_len)\n",
        "        Output shape: (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        self.inputs = inputs\n",
        "        return self.params['W'][inputs]\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"\n",
        "        Gradient shape: (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # The gradient of the embedding matrix W is the sum of the gradients\n",
        "        # for each word that appeared in the input.\n",
        "        # Np.add.at is used for efficient in-place addition at specific indices.\n",
        "        np.add.at(self.grads['W'], self.inputs, grad)\n",
        "        return None # This is the first layer, so no gradient to pass back"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "221ef1bb-2935-4816-a22f-8a390498a47e",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "221ef1bb-2935-4816-a22f-8a390498a47e"
      },
      "source": [
        "#### Dense (Fully-Connected) Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "ddc5a4f2-e31a-4ff6-8f39-93759f765aad",
      "metadata": {
        "id": "ddc5a4f2-e31a-4ff6-8f39-93759f765aad"
      },
      "outputs": [],
      "source": [
        "class Dense(Layer):\n",
        "    \"\"\"\n",
        "    A fully-connected layer.\n",
        "    Updated to handle both 2D and 3D input tensors.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super().__init__()\n",
        "        # Xavier Glorot initialization\n",
        "        limit = np.sqrt(6 / (input_size + output_size))\n",
        "        self.params['W'] = np.random.uniform(-limit, limit, (input_size, output_size))\n",
        "        self.params['b'] = np.zeros(output_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Inputs shape: (batch_size, ..., input_size)\n",
        "        Output shape: (batch_size, ..., output_size)\n",
        "        \"\"\"\n",
        "        self.inputs = inputs\n",
        "        return inputs @ self.params['W'] + self.params['b']\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"\n",
        "        grad shape: (batch_size, ..., output_size)\n",
        "        Handles both 2D and 3D cases for Transformer compatibility.\n",
        "        \"\"\"\n",
        "        # For the bias gradient, sum over all dimensions except the last one (features).\n",
        "        sum_axes = tuple(range(grad.ndim - 1))\n",
        "        self.grads['b'] = np.sum(grad, axis=sum_axes)\n",
        "\n",
        "        # For the weight gradient, we need (D_in, N) @ (N, D_out)\n",
        "        # If input is 3D (N, S, D_in), we reshape to (N*S, D_in)\n",
        "        if self.inputs.ndim == 3:\n",
        "            N, S, D_in = self.inputs.shape\n",
        "            D_out = grad.shape[-1]\n",
        "            # Reshape inputs and grad to be 2D for the matmul\n",
        "            inputs_reshaped = self.inputs.reshape(N * S, D_in)\n",
        "            grad_reshaped = grad.reshape(N * S, D_out)\n",
        "            self.grads['W'] = inputs_reshaped.T @ grad_reshaped\n",
        "        else: # Original 2D case for RNN/LSTM and the final Dense layer in Transformer\n",
        "            self.grads['W'] = self.inputs.T @ grad\n",
        "\n",
        "        # The gradient w.r.t input is still a simple matmul\n",
        "        return grad @ self.params['W'].T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "009f0af5-3a87-4069-9178-7d7e86cc731d",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "009f0af5-3a87-4069-9178-7d7e86cc731d"
      },
      "source": [
        "#### 3. Loss Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3f2f35bc-75b8-43ec-a4ab-c7018a079c17",
      "metadata": {
        "id": "3f2f35bc-75b8-43ec-a4ab-c7018a079c17"
      },
      "outputs": [],
      "source": [
        "class SoftmaxCrossEntropy:\n",
        "    \"\"\"\n",
        "    Computes the cross-entropy loss after applying softmax.\n",
        "    y_pred is expected to be logits (raw outputs from the last dense layer).\n",
        "    y_true is expected to be integer class labels.\n",
        "    \"\"\"\n",
        "    def forward(self, y_pred, y_true):\n",
        "        \"\"\"\n",
        "        y_pred shape: (batch_size, num_classes)\n",
        "        y_true shape: (batch_size,)\n",
        "        \"\"\"\n",
        "        self.y_true = y_true\n",
        "        batch_size = y_pred.shape[0]\n",
        "\n",
        "        # Stabilize softmax by subtracting the max logit\n",
        "        exp_preds = np.exp(y_pred - np.max(y_pred, axis=1, keepdims=True))\n",
        "        self.probs = exp_preds / np.sum(exp_preds, axis=1, keepdims=True)\n",
        "\n",
        "        # Calculate loss\n",
        "        log_likelihood = -np.log(self.probs[range(batch_size), y_true])\n",
        "        loss = np.sum(log_likelihood) / batch_size\n",
        "        return loss\n",
        "\n",
        "    def backward(self):\n",
        "        \"\"\"\n",
        "        Calculates the gradient of the loss with respect to y_pred (the logits).\n",
        "        \"\"\"\n",
        "        batch_size = self.probs.shape[0]\n",
        "        grad = self.probs.copy()\n",
        "\n",
        "        # The gradient is simply (probs - 1) for the correct class\n",
        "        grad[range(batch_size), self.y_true] -= 1\n",
        "        grad /= batch_size\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cef627c-09fe-4846-9913-d58a8be28c47",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "3cef627c-09fe-4846-9913-d58a8be28c47"
      },
      "source": [
        "#### 4. The Adam Optimizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "cd8fb978-1d89-47de-b261-564cf4b9615e",
      "metadata": {
        "id": "cd8fb978-1d89-47de-b261-564cf4b9615e"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    \"\"\"\n",
        "    The Adam optimizer with gradient clipping.\n",
        "    \"\"\"\n",
        "    def __init__(self, layers, learning_rate=0.001, beta1=0.9, beta2=0.999,\n",
        "                 epsilon=1e-8, clip_value=1.0, clip_norm=None):\n",
        "        self.layers = layers\n",
        "        self.lr = learning_rate\n",
        "        self.beta1 = beta1\n",
        "        self.beta2 = beta2\n",
        "        self.epsilon = epsilon\n",
        "        self.clip_value = clip_value  # Value clipping threshold\n",
        "        self.clip_norm = clip_norm    # Gradient norm clipping threshold\n",
        "        self.t = 0\n",
        "\n",
        "        # Initialize moment vectors for each parameter in each layer\n",
        "        self.m = {}\n",
        "        self.v = {}\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Only initialize moments for layers that have parameters\n",
        "            if hasattr(layer, 'params'):\n",
        "                for key in layer.params:\n",
        "                    param_key = f'layer_{i}_{key}'\n",
        "                    self.m[param_key] = np.zeros_like(layer.params[key])\n",
        "                    self.v[param_key] = np.zeros_like(layer.params[key])\n",
        "\n",
        "    def _clip_gradients(self, grads):\n",
        "        \"\"\"\n",
        "        Apply gradient clipping to prevent exploding gradients.\n",
        "        Supports both value clipping and norm clipping.\n",
        "        \"\"\"\n",
        "        if self.clip_value is not None:\n",
        "            # Value clipping: clip each gradient element individually\n",
        "            grads = np.clip(grads, -self.clip_value, self.clip_value)\n",
        "\n",
        "        if self.clip_norm is not None:\n",
        "            # Norm clipping: scale gradients if their norm exceeds threshold\n",
        "            grad_norm = np.linalg.norm(grads)\n",
        "            if grad_norm > self.clip_norm:\n",
        "                grads = grads * (self.clip_norm / (grad_norm + self.epsilon))\n",
        "\n",
        "        return grads\n",
        "\n",
        "    def step(self):\n",
        "        \"\"\"\n",
        "        Performs a single optimization step with gradient clipping.\n",
        "        \"\"\"\n",
        "        self.t += 1\n",
        "\n",
        "        for i, layer in enumerate(self.layers):\n",
        "            # Only update parameters for layers that have them\n",
        "            if hasattr(layer, 'params'):\n",
        "                for key, param in layer.params.items():\n",
        "                    param_key = f'layer_{i}_{key}'\n",
        "\n",
        "                    # Ensure the layer has gradients for this parameter\n",
        "                    if key in layer.grads:\n",
        "                        grad = layer.grads[key].copy()  # Make a copy to avoid modifying original\n",
        "\n",
        "                        # Apply gradient clipping\n",
        "                        grad = self._clip_gradients(grad)\n",
        "\n",
        "                        # Update biased first moment estimate\n",
        "                        self.m[param_key] = self.beta1 * self.m[param_key] + (1 - self.beta1) * grad\n",
        "\n",
        "                        # Update biased second raw moment estimate\n",
        "                        self.v[param_key] = self.beta2 * self.v[param_key] + (1 - self.beta2) * (grad**2)\n",
        "\n",
        "                        # Compute bias-corrected first moment estimate\n",
        "                        m_hat = self.m[param_key] / (1 - self.beta1**self.t)\n",
        "\n",
        "                        # Compute bias-corrected second raw moment estimate\n",
        "                        v_hat = self.v[param_key] / (1 - self.beta2**self.t)\n",
        "\n",
        "                        # Update parameters\n",
        "                        update = self.lr * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
        "                        param -= update\n",
        "\n",
        "    def zero_grad(self):\n",
        "        \"\"\"\n",
        "        Reset gradients for all layers (optional but good practice)\n",
        "        \"\"\"\n",
        "        for layer in self.layers:\n",
        "            if hasattr(layer, 'grads'):\n",
        "                for key in layer.grads:\n",
        "                    layer.grads[key] = np.zeros_like(layer.grads[key])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "585e813a-b4be-4361-be0a-7ff21eb53043",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "585e813a-b4be-4361-be0a-7ff21eb53043"
      },
      "source": [
        "#### 5. Classification Report Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "3b206f5d-1677-43da-b9b9-d39cfb01546a",
      "metadata": {
        "id": "3b206f5d-1677-43da-b9b9-d39cfb01546a"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def classification_report_from_scratch(y_true, y_pred, class_names=None):\n",
        "    \"\"\"\n",
        "    Generates and prints a text report showing the main classification metrics.\n",
        "    Now includes micro, macro, and weighted averages for F1-score.\n",
        "    \"\"\"\n",
        "    # Get unique class labels, sorted for consistent ordering\n",
        "    unique_labels = sorted(np.unique(np.concatenate((y_true, y_pred))))\n",
        "\n",
        "    if class_names is None:\n",
        "        class_names = [f\"Class {label}\" for label in unique_labels]\n",
        "\n",
        "    max_name_len = max([len(name) for name in class_names] + [len(\"weighted avg\")])\n",
        "\n",
        "    # --- Print Header ---\n",
        "    header = (\n",
        "        f\"{'':<{max_name_len}}  \"\n",
        "        f\"{'precision':>10}  {'recall':>10}  {'f1-score':>10}  {'support':>10}\\n\\n\"\n",
        "    )\n",
        "    report_str = header\n",
        "\n",
        "    # Initialize for calculations\n",
        "    precisions, recalls, f1_scores, supports = [], [], [], []\n",
        "    total_tp, total_fp, total_fn = 0, 0, 0\n",
        "\n",
        "    # --- Calculate and format metrics for each class ---\n",
        "    for i, label in enumerate(unique_labels):\n",
        "        true_class = (y_true == label)\n",
        "        pred_class = (y_pred == label)\n",
        "\n",
        "        tp = np.sum(true_class & pred_class)\n",
        "        fp = np.sum(~true_class & pred_class)\n",
        "        fn = np.sum(true_class & ~pred_class)\n",
        "\n",
        "        support = np.sum(true_class)\n",
        "\n",
        "        # Aggregate totals for micro-average calculation\n",
        "        total_tp += tp\n",
        "        total_fp += fp\n",
        "        total_fn += fn\n",
        "\n",
        "        epsilon = 1e-8\n",
        "        precision = tp / (tp + fp + epsilon)\n",
        "        recall = tp / (tp + fn + epsilon)\n",
        "        f1_score = 2 * (precision * recall) / (precision + recall + epsilon)\n",
        "\n",
        "        precisions.append(precision)\n",
        "        recalls.append(recall)\n",
        "        f1_scores.append(f1_score)\n",
        "        supports.append(support)\n",
        "\n",
        "        report_str += (\n",
        "            f\"{class_names[i]:<{max_name_len}}  \"\n",
        "            f\"{precision:>10.2f}  \"\n",
        "            f\"{recall:>10.2f}  \"\n",
        "            f\"{f1_score:>10.2f}  \"\n",
        "            f\"{support:>10}\\n\"\n",
        "        )\n",
        "\n",
        "    report_str += \"\\n\"\n",
        "    total_samples = np.sum(supports)\n",
        "\n",
        "    # --- Calculate and format overall and average metrics ---\n",
        "    accuracy = np.sum(y_true == y_pred) / total_samples\n",
        "\n",
        "    # Accuracy\n",
        "    report_str += (\n",
        "        f\"{'accuracy':<{max_name_len}}  \"\n",
        "        f\"{'':>10}  {'':>10}  \"\n",
        "        f\"{accuracy:>10.2f}  {total_samples:>10}\\n\"\n",
        "    )\n",
        "\n",
        "    # --- NEW: Micro Average Calculation ---\n",
        "    # Note: In multiclass, micro-precision, micro-recall, and micro-f1 are all equal to accuracy.\n",
        "    micro_precision = total_tp / (total_tp + total_fp + 1e-8)\n",
        "    micro_recall = total_tp / (total_tp + total_fn + 1e-8)\n",
        "    micro_f1 = 2 * (micro_precision * micro_recall) / (micro_precision + micro_recall + 1e-8)\n",
        "    report_str += (\n",
        "        f\"{'micro avg':<{max_name_len}}  \"\n",
        "        f\"{micro_precision:>10.2f}  \"\n",
        "        f\"{micro_recall:>10.2f}  \"\n",
        "        f\"{micro_f1:>10.2f}  \"\n",
        "        f\"{total_samples:>10}\\n\"\n",
        "    )\n",
        "\n",
        "    # Macro average\n",
        "    report_str += (\n",
        "        f\"{'macro avg':<{max_name_len}}  \"\n",
        "        f\"{np.mean(precisions):>10.2f}  \"\n",
        "        f\"{np.mean(recalls):>10.2f}  \"\n",
        "        f\"{np.mean(f1_scores):>10.2f}  \"\n",
        "        f\"{total_samples:>10}\\n\"\n",
        "    )\n",
        "\n",
        "    # Weighted average\n",
        "    report_str += (\n",
        "        f\"{'weighted avg':<{max_name_len}}  \"\n",
        "        f\"{np.sum(np.array(precisions) * np.array(supports)) / total_samples:>10.2f}  \"\n",
        "        f\"{np.sum(np.array(recalls) * np.array(supports)) / total_samples:>10.2f}  \"\n",
        "        f\"{np.sum(np.array(f1_scores) * np.array(supports)) / total_samples:>10.2f}  \"\n",
        "        f\"{total_samples:>10}\\n\"\n",
        "    )\n",
        "\n",
        "    print(report_str)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1f9b0e9-7d08-4e3c-b1c1-ebe63b129fce",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "a1f9b0e9-7d08-4e3c-b1c1-ebe63b129fce"
      },
      "source": [
        "#### 6. Dropout Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "72b84eaa-29ff-4152-bb8f-e5ea64720fe3",
      "metadata": {
        "id": "72b84eaa-29ff-4152-bb8f-e5ea64720fe3"
      },
      "outputs": [],
      "source": [
        "class Dropout(Layer):\n",
        "    \"\"\"\n",
        "    Implements the Dropout layer for regularization.\n",
        "    \"\"\"\n",
        "    def __init__(self, p=0.5):\n",
        "        super().__init__()\n",
        "        self.p = p\n",
        "        self.mask = None\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        \"\"\"\n",
        "        Applies dropout during training.\n",
        "        During evaluation (training=False), it does nothing.\n",
        "        \"\"\"\n",
        "        if training:\n",
        "            # Create a mask and apply inverted dropout\n",
        "            # We scale the outputs by 1/(1-p) during training\n",
        "            self.mask = (np.random.rand(*x.shape) > self.p) / (1 - self.p)\n",
        "            return x * self.mask\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"\n",
        "        Applies the same mask to the gradients.\n",
        "        \"\"\"\n",
        "        # Gradients only flow through the neurons that were not dropped out\n",
        "        return grad * self.mask"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5ef2e80-07fc-4fcf-bc81-06b6772e8604",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "a5ef2e80-07fc-4fcf-bc81-06b6772e8604"
      },
      "source": [
        "#### 7. Save the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "66109727-373a-4626-a54d-673248dae636",
      "metadata": {
        "id": "66109727-373a-4626-a54d-673248dae636"
      },
      "outputs": [],
      "source": [
        "# --- Saving Weights while Training ---\n",
        "\n",
        "def save_weights(model, path):\n",
        "    \"\"\"Saves model weights to a file.\"\"\"\n",
        "    weights = {}\n",
        "    for i, layer in enumerate(model.layers):\n",
        "        for key, param in layer.params.items():\n",
        "            weights[f'layer_{i}_{key}'] = param\n",
        "    np.savez(path, **weights)\n",
        "    print(f\"Weights saved to {path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb54f7e1-2c98-4bfa-aa50-b8fed822c6a9",
      "metadata": {
        "id": "fb54f7e1-2c98-4bfa-aa50-b8fed822c6a9"
      },
      "source": [
        "## Part 3. Model 1 - RNN from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4faa6bab-b4fd-490e-8624-9a08a9e9c8a3",
      "metadata": {
        "id": "4faa6bab-b4fd-490e-8624-9a08a9e9c8a3"
      },
      "source": [
        "### 1. Simple RNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "63788a19-1a28-4a86-96ab-30e99fac2e22",
      "metadata": {
        "id": "63788a19-1a28-4a86-96ab-30e99fac2e22"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Note: The classes from Part 2 (Layer, Tanh, Embedding, Dense, etc.) are assumed to be defined.\n",
        "\n",
        "class SimpleRNN(Layer):\n",
        "    \"\"\"\n",
        "    A simple Recurrent Neural Network layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Xavier initialization\n",
        "        xavier_std = np.sqrt(2.0 / (input_size + hidden_size))\n",
        "        self.params['W_xh'] = np.random.randn(input_size, hidden_size) * xavier_std\n",
        "        self.params['W_hh'] = np.random.randn(hidden_size, hidden_size) * 0.01  # Smaller for recurrent weights\n",
        "        self.params['b_h'] = np.zeros(hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        \"\"\"\n",
        "        Processes a sequence of inputs.\n",
        "        Inputs shape: (batch_size, seq_len, input_size)\n",
        "        Output shape: (batch_size, hidden_size) -> The final hidden state\n",
        "        \"\"\"\n",
        "        self.inputs = inputs\n",
        "        batch_size, seq_len, _ = inputs.shape\n",
        "\n",
        "        # Initialize hidden state and storage for backprop\n",
        "        self.h_states = np.zeros((batch_size, seq_len + 1, self.hidden_size))\n",
        "        self.tanh_inputs = np.zeros((batch_size, seq_len, self.hidden_size))\n",
        "\n",
        "        # Initial hidden state h_0 is all zeros\n",
        "        h = self.h_states[:, 0, :]\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = inputs[:, t, :]\n",
        "            pre_activation = x_t @ self.params['W_xh'] + h @ self.params['W_hh'] + self.params['b_h']\n",
        "            self.tanh_inputs[:, t, :] = pre_activation\n",
        "            h = np.tanh(pre_activation)\n",
        "            self.h_states[:, t+1, :] = h\n",
        "\n",
        "        return h # Return the final hidden state\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"\n",
        "        Performs Backpropagation Through Time (BPTT).\n",
        "        grad shape: (batch_size, hidden_size) -> gradient of the final hidden state\n",
        "        Returns gradient w.r.t inputs: (batch_size, seq_len, input_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = self.inputs.shape\n",
        "\n",
        "        # Initialize gradients for parameters and input\n",
        "        self.grads['W_xh'] = np.zeros_like(self.params['W_xh'])\n",
        "        self.grads['W_hh'] = np.zeros_like(self.params['W_hh'])\n",
        "        self.grads['b_h'] = np.zeros_like(self.params['b_h'])\n",
        "        grad_inputs = np.zeros_like(self.inputs)\n",
        "\n",
        "        # Gradient of the hidden state, starting from the end and flowing backwards\n",
        "        grad_h_next = grad\n",
        "\n",
        "        for t in reversed(range(seq_len)):\n",
        "            # Gradient through the tanh activation\n",
        "            grad_tanh = (1 - self.h_states[:, t+1, :]**2) * grad_h_next\n",
        "\n",
        "            # Gradients for parameters at this time step\n",
        "            self.grads['b_h'] += np.sum(grad_tanh, axis=0)\n",
        "            self.grads['W_xh'] += self.inputs[:, t, :].T @ grad_tanh\n",
        "            self.grads['W_hh'] += self.h_states[:, t, :].T @ grad_tanh # Use h_{t-1}\n",
        "\n",
        "            # Gradient to pass to the previous hidden state\n",
        "            grad_h_prev = grad_tanh @ self.params['W_hh'].T\n",
        "\n",
        "            # Gradient for the input at this time step\n",
        "            grad_inputs[:, t, :] = grad_tanh @ self.params['W_xh'].T\n",
        "\n",
        "            # Update the hidden state gradient for the next iteration (t-1)\n",
        "            grad_h_next = grad_h_prev\n",
        "\n",
        "        return grad_inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "668ec24e-e733-4e9c-ae59-52f75a916ec9",
      "metadata": {
        "id": "668ec24e-e733-4e9c-ae59-52f75a916ec9"
      },
      "source": [
        "### 2. Assembling the Full RNN Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "c267e396-ce4f-4e37-8ad6-8fbf9aead7b1",
      "metadata": {
        "id": "c267e396-ce4f-4e37-8ad6-8fbf9aead7b1"
      },
      "outputs": [],
      "source": [
        "class RNNClassifier:\n",
        "    \"\"\"\n",
        "    A complete RNN model for classification.\n",
        "    (Your __init__ and other methods remain the same)\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes, dropout_p=0.3):\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.rnn = SimpleRNN(embed_dim, hidden_size)\n",
        "        self.dropout = Dropout(dropout_p)\n",
        "        self.dense = Dense(hidden_size, num_classes)\n",
        "        self.layers = [self.embedding, self.rnn, self.dropout, self.dense]\n",
        "\n",
        "    def forward(self, inputs, training=True):\n",
        "        \"\"\"\n",
        "        Forward pass for the classifier.\n",
        "        The 'training' flag controls the dropout layer.\n",
        "        \"\"\"\n",
        "        x = self.embedding.forward(inputs)\n",
        "        x = self.rnn.forward(x)\n",
        "        # Apply dropout only during training\n",
        "        x = self.dropout.forward(x, training=training)\n",
        "        x = self.dense.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"\n",
        "        Backward pass for the classifier.\n",
        "        \"\"\"\n",
        "        grad = self.dense.backward(grad)\n",
        "        # Add the backward pass for the dropout layer\n",
        "        grad = self.dropout.backward(grad)\n",
        "        grad = self.rnn.backward(grad)\n",
        "        self.embedding.backward(grad)\n",
        "\n",
        "    def get_params(self):\n",
        "        params = []\n",
        "        for layer in self.layers:\n",
        "            params.extend(layer.params.values())\n",
        "        return params\n",
        "\n",
        "    def get_grads(self):\n",
        "        grads = []\n",
        "        for layer in self.layers:\n",
        "            grads.extend(layer.grads.values())\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6f2da7f-13ba-424f-9165-d9b53fd7bd7c",
      "metadata": {
        "id": "d6f2da7f-13ba-424f-9165-d9b53fd7bd7c"
      },
      "source": [
        "### 3. The Training Loop and Weight Management"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "1a60baba-2bd3-4137-a638-83bc14bcb897",
      "metadata": {
        "id": "1a60baba-2bd3-4137-a638-83bc14bcb897"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import requests\n",
        "# import io\n",
        "# from tqdm import tqdm # For a nice progress bar\n",
        "\n",
        "\n",
        "# # --- Model and Training Hyperparameters ---\n",
        "\n",
        "# VOCAB_SIZE = len(vocab)\n",
        "# EMBED_DIM = 100 # As required by the assignment\n",
        "# HIDDEN_SIZE = 256\n",
        "# NUM_CLASSES = 5\n",
        "# EPOCHS = 10 # A few epochs for demonstration\n",
        "# BATCH_SIZE = 32\n",
        "# LEARNING_RATE = 0.0005\n",
        "\n",
        "# # --- Initialization ---\n",
        "\n",
        "# model = RNNClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_CLASSES)\n",
        "# loss_fn = SoftmaxCrossEntropy()\n",
        "# optimizer = Adam(\n",
        "#     model.layers,\n",
        "#     learning_rate=0.0005,\n",
        "#     beta1=0.9,\n",
        "#     beta2=0.999,\n",
        "#     epsilon=1e-8,\n",
        "#     clip_value=1.0,      # Clip gradient values between -1.0 and 1.0\n",
        "#     clip_norm=5.0        # Additionally clip gradient norm to 5.0\n",
        "# )\n",
        "\n",
        "# # --- Training Loop ---\n",
        "\n",
        "# num_batches = len(X_train) // BATCH_SIZE\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     epoch_loss = 0\n",
        "#     # Shuffle training data\n",
        "#     permutation = np.random.permutation(len(X_train))\n",
        "#     X_train_shuffled = X_train[permutation]\n",
        "#     y_train_shuffled = y_train[permutation]\n",
        "\n",
        "#     # Create a progress bar\n",
        "#     pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{EPOCHS}\")\n",
        "\n",
        "#     for i in pbar:\n",
        "#         # Create a mini-batch\n",
        "#         start = i * BATCH_SIZE\n",
        "#         end = start + BATCH_SIZE\n",
        "#         X_batch = X_train_shuffled[start:end]\n",
        "#         y_batch = y_train_shuffled[start:end]\n",
        "\n",
        "#         # 1. Forward pass\n",
        "#         logits = model.forward(X_batch, training=True)\n",
        "\n",
        "#         # 2. Compute loss\n",
        "#         loss = loss_fn.forward(logits, y_batch)\n",
        "#         epoch_loss += loss\n",
        "\n",
        "#         # 3. Backward pass\n",
        "#         grad = loss_fn.backward()\n",
        "#         model.backward(grad)\n",
        "\n",
        "#         # 4. Update weights\n",
        "#         optimizer.step()\n",
        "\n",
        "#         # Update progress bar description\n",
        "#         pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
        "\n",
        "#     print(f\"Epoch {epoch+1} Average Loss: {epoch_loss / num_batches:.4f}\")\n",
        "\n",
        "# # --- Save the trained weights ---\n",
        "# if not os.path.exists('saved_weights'):\n",
        "#     os.makedirs('saved_weights')\n",
        "\n",
        "# save_weights(model, 'saved_weights/rnn_model_weights.npz')\n",
        "\n",
        "# # --- Evaluation ---\n",
        "# def evaluate(model, X, y):\n",
        "#     logits = model.forward(X, training=False)\n",
        "#     predictions = np.argmax(logits, axis=1)\n",
        "#     accuracy = np.mean(predictions == y)\n",
        "#     return accuracy\n",
        "\n",
        "# train_accuracy = evaluate(model, X_train[:500], y_train[:500]) # On a subset for speed\n",
        "# test_accuracy = evaluate(model, X_test, y_test)\n",
        "# print(f\"\\nTraining Accuracy: {train_accuracy:.4f}\")\n",
        "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b33aa0dc-04ac-4151-b443-ab02bdefe74f",
      "metadata": {
        "id": "b33aa0dc-04ac-4151-b443-ab02bdefe74f"
      },
      "source": [
        "### 4. Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "f2e958ba-ad10-4146-98ae-686a945646d7",
      "metadata": {
        "id": "f2e958ba-ad10-4146-98ae-686a945646d7"
      },
      "outputs": [],
      "source": [
        "# print(\"Evaluating on the test set...\")\n",
        "# logits_test = model.forward(X_test, training=False)\n",
        "# predictions_test = np.argmax(logits_test, axis=1)\n",
        "\n",
        "# # You can define human-readable names for your classes\n",
        "# # The number of names should match NUM_CLASSES\n",
        "# class_labels = ['Neutral','Positive','Extremely Negative','Negative','Extremely Positive']\n",
        "\n",
        "# # Generate and print the report\n",
        "# print(\"\\n--- Classification Report ---\")\n",
        "# classification_report_from_scratch(y_test, predictions_test, class_names=class_labels)\n",
        "\n",
        "\n",
        "# test_accuracy = np.mean(predictions_test == y_test)\n",
        "# print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5. Model 2 - LSTM from scratch"
      ],
      "metadata": {
        "id": "ECDJixutlB2B"
      },
      "id": "ECDJixutlB2B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. LSTM Layer"
      ],
      "metadata": {
        "id": "WTemaoIXlIMR"
      },
      "id": "WTemaoIXlIMR"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "3392e0c2-9d70-4662-876e-81fc671b3855",
      "metadata": {
        "id": "3392e0c2-9d70-4662-876e-81fc671b3855"
      },
      "outputs": [],
      "source": [
        "class LSTM(Layer):\n",
        "    \"\"\"A Long Short-Term Memory (LSTM) layer.\"\"\"\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super().__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # We concatenate weights for all 4 gates for efficiency\n",
        "        # W_x maps input to the 4 gates, W_h maps hidden state to the 4 gates\n",
        "        size_sum = input_size + hidden_size\n",
        "        limit_x = np.sqrt(6 / (size_sum))\n",
        "        limit_h = np.sqrt(6 / (hidden_size + hidden_size))\n",
        "\n",
        "        self.params['W_x'] = np.random.uniform(-limit_x, limit_x, (input_size, 4 * hidden_size))\n",
        "        self.params['W_h'] = np.random.uniform(-limit_h, limit_h, (hidden_size, 4 * hidden_size))\n",
        "        self.params['b'] = np.zeros(4 * hidden_size)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        self.inputs = inputs\n",
        "        batch_size, seq_len, _ = inputs.shape\n",
        "        h_size = self.hidden_size\n",
        "\n",
        "        # Caches for backpropagation\n",
        "        self.h_states = np.zeros((batch_size, seq_len + 1, h_size))\n",
        "        self.c_states = np.zeros((batch_size, seq_len + 1, h_size))\n",
        "        self.gates_pre = np.zeros((batch_size, seq_len, 4 * h_size))\n",
        "        self.gates = np.zeros((batch_size, seq_len, 4 * h_size))\n",
        "\n",
        "        # Initial hidden and cell states are zeros\n",
        "        h = self.h_states[:, 0, :]\n",
        "        c = self.c_states[:, 0, :]\n",
        "\n",
        "        for t in range(seq_len):\n",
        "            x_t = inputs[:, t, :]\n",
        "\n",
        "            # Combined matrix multiplication for all gates\n",
        "            pre_activation = x_t @ self.params['W_x'] + h @ self.params['W_h'] + self.params['b']\n",
        "            self.gates_pre[:, t, :] = pre_activation\n",
        "\n",
        "            # Split into individual gates\n",
        "            f_gate = self._sigmoid(pre_activation[:, :h_size])\n",
        "            i_gate = self._sigmoid(pre_activation[:, h_size:2*h_size])\n",
        "            g_gate = np.tanh(pre_activation[:, 2*h_size:3*h_size])\n",
        "            o_gate = self._sigmoid(pre_activation[:, 3*h_size:])\n",
        "            self.gates[:, t, :] = np.hstack((f_gate, i_gate, g_gate, o_gate))\n",
        "\n",
        "            # Update cell and hidden states\n",
        "            c = f_gate * c + i_gate * g_gate\n",
        "            h = o_gate * np.tanh(c)\n",
        "\n",
        "            self.c_states[:, t+1, :] = c\n",
        "            self.h_states[:, t+1, :] = h\n",
        "\n",
        "        return h\n",
        "\n",
        "    def backward(self, grad_h_final):\n",
        "        batch_size, seq_len, _ = self.inputs.shape\n",
        "        h_size = self.hidden_size\n",
        "\n",
        "        # Initialize gradients\n",
        "        self.grads['W_x'] = np.zeros_like(self.params['W_x'])\n",
        "        self.grads['W_h'] = np.zeros_like(self.params['W_h'])\n",
        "        self.grads['b'] = np.zeros_like(self.params['b'])\n",
        "        grad_inputs = np.zeros_like(self.inputs)\n",
        "\n",
        "        # Initialize gradients for hidden and cell states flowing backwards\n",
        "        grad_h = grad_h_final\n",
        "        grad_c = np.zeros_like(grad_h)\n",
        "\n",
        "        for t in reversed(range(seq_len)):\n",
        "            # Retrieve cached values for this timestep\n",
        "            h_prev = self.h_states[:, t, :]\n",
        "            c_prev = self.c_states[:, t, :]\n",
        "            c_t = self.c_states[:, t+1, :]\n",
        "            x_t = self.inputs[:, t, :]\n",
        "\n",
        "            f_t, i_t, g_t, o_t = (\n",
        "                self.gates[:, t, :h_size],\n",
        "                self.gates[:, t, h_size:2*h_size],\n",
        "                self.gates[:, t, 2*h_size:3*h_size],\n",
        "                self.gates[:, t, 3*h_size:]\n",
        "            )\n",
        "\n",
        "            # Backprop through h_t = o_t * tanh(c_t)\n",
        "            grad_o = grad_h * np.tanh(c_t)\n",
        "            grad_c += grad_h * o_t * (1 - np.tanh(c_t)**2)\n",
        "\n",
        "            # Backprop through c_t = f_t * c_prev + i_t * g_t\n",
        "            grad_f = grad_c * c_prev\n",
        "            grad_i = grad_c * g_t\n",
        "            grad_g = grad_c * i_t\n",
        "            grad_c_prev = grad_c * f_t\n",
        "\n",
        "            # Backprop through gate activations\n",
        "            d_o_pre = grad_o * o_t * (1 - o_t) # Sigmoid derivative\n",
        "            d_g_pre = grad_g * (1 - g_t**2)      # Tanh derivative\n",
        "            d_i_pre = grad_i * i_t * (1 - i_t) # Sigmoid derivative\n",
        "            d_f_pre = grad_f * f_t * (1 - f_t) # Sigmoid derivative\n",
        "\n",
        "            # Concatenate gate gradients\n",
        "            d_gates = np.hstack((d_f_pre, d_i_pre, d_g_pre, d_o_pre))\n",
        "\n",
        "            # Calculate gradients for parameters and inputs\n",
        "            self.grads['b'] += np.sum(d_gates, axis=0)\n",
        "            self.grads['W_x'] += x_t.T @ d_gates\n",
        "            self.grads['W_h'] += h_prev.T @ d_gates\n",
        "\n",
        "            grad_inputs[:, t, :] = d_gates @ self.params['W_x'].T\n",
        "\n",
        "            # Update gradients for the next (previous) timestep\n",
        "            grad_h = d_gates @ self.params['W_h'].T\n",
        "            grad_c = grad_c_prev\n",
        "\n",
        "        return grad_inputs\n",
        "\n",
        "    def _sigmoid(self, x):\n",
        "        return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37c3e5c9-6406-4160-bfaf-9dd615a695a7",
      "metadata": {
        "id": "37c3e5c9-6406-4160-bfaf-9dd615a695a7"
      },
      "source": [
        "### 2. Assembling LSTM Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "3d4fb9b0-8479-43d7-ac04-2f1707b81994",
      "metadata": {
        "id": "3d4fb9b0-8479-43d7-ac04-2f1707b81994"
      },
      "outputs": [],
      "source": [
        "class LSTMClassifier:\n",
        "    \"\"\"A complete LSTM model for classification.\"\"\"\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_size, num_classes):\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.lstm = LSTM(embed_dim, hidden_size)\n",
        "        self.dense = Dense(hidden_size, num_classes)\n",
        "        self.layers = [self.embedding, self.lstm, self.dense]\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        x = self.embedding.forward(inputs)\n",
        "        x = self.lstm.forward(x)\n",
        "        x = self.dense.forward(x)\n",
        "        return x\n",
        "\n",
        "    def backward(self, grad):\n",
        "        grad = self.dense.backward(grad)\n",
        "        grad = self.lstm.backward(grad)\n",
        "        self.embedding.backward(grad)\n",
        "\n",
        "# --- Model and Training Hyperparameters ---\n",
        "# Same as before, but we can give the model a new name\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_CLASSES = 5\n",
        "EPOCHS = 5\n",
        "BATCH_SIZE = 64\n",
        "LEARNING_RATE = 0.001\n",
        "\n",
        "# --- Initialization ---\n",
        "lstm_model = LSTMClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_CLASSES)\n",
        "loss_fn = SoftmaxCrossEntropy()\n",
        "optimizer = Adam(lstm_model.layers, learning_rate=LEARNING_RATE)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Training the LSTM Classifier"
      ],
      "metadata": {
        "id": "qEBlVh7g4lmv"
      },
      "id": "qEBlVh7g4lmv"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "f0c66088-6a2c-44ff-8497-1c4df25ab57f",
      "metadata": {
        "id": "f0c66088-6a2c-44ff-8497-1c4df25ab57f"
      },
      "outputs": [],
      "source": [
        "# # --- Training Loop (This code is identical to the RNN training loop) ---\n",
        "# num_batches = len(X_train) // BATCH_SIZE\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     epoch_loss = 0\n",
        "#     permutation = np.random.permutation(len(X_train))\n",
        "#     X_train_shuffled = X_train[permutation]\n",
        "#     y_train_shuffled = y_train[permutation]\n",
        "\n",
        "#     pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{EPOCHS} (LSTM)\")\n",
        "\n",
        "#     for i in pbar:\n",
        "#         start = i * BATCH_SIZE\n",
        "#         end = start + BATCH_SIZE\n",
        "#         X_batch, y_batch = X_train_shuffled[start:end], y_train_shuffled[start:end]\n",
        "\n",
        "#         logits = lstm_model.forward(X_batch)\n",
        "#         loss = loss_fn.forward(logits, y_batch)\n",
        "#         epoch_loss += loss\n",
        "\n",
        "#         grad = loss_fn.backward()\n",
        "#         lstm_model.backward(grad)\n",
        "#         optimizer.step()\n",
        "\n",
        "#         pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
        "\n",
        "#     print(f\"Epoch {epoch+1} Average Loss: {epoch_loss / num_batches:.4f}\")\n",
        "\n",
        "# # --- Save and Evaluate ---\n",
        "# save_weights(lstm_model, 'saved_weights/lstm_model_weights.npz')\n",
        "\n",
        "# # --- Evaluation ---\n",
        "# def evaluate(model, X, y):\n",
        "#     logits = model.forward(X)\n",
        "#     predictions = np.argmax(logits, axis=1)\n",
        "#     accuracy = np.mean(predictions == y)\n",
        "#     return accuracy\n",
        "\n",
        "# print(\"\\n--- LSTM Model Evaluation ---\")\n",
        "# train_accuracy = evaluate(lstm_model, X_train[:500], y_train[:500])\n",
        "# test_accuracy = evaluate(lstm_model, X_test, y_test)\n",
        "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690494c9-ab76-4ad1-9042-58d2a68df982",
      "metadata": {
        "id": "690494c9-ab76-4ad1-9042-58d2a68df982"
      },
      "source": [
        "### 3. Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "8082dfce-4569-41d4-87c6-bcd82d6ab0e5",
      "metadata": {
        "id": "8082dfce-4569-41d4-87c6-bcd82d6ab0e5"
      },
      "outputs": [],
      "source": [
        "# print(\"Evaluating on the test set...\")\n",
        "# logits_test = lstm_model.forward(X_test)\n",
        "# predictions_test = np.argmax(logits_test, axis=1)\n",
        "\n",
        "# # You can define human-readable names for your classes\n",
        "# # The number of names should match NUM_CLASSES\n",
        "# class_labels = ['Neutral','Positive','Extremely Negative','Negative','Extremely Positive']\n",
        "\n",
        "# # Generate and print the report\n",
        "# print(\"\\n--- Classification Report ---\")\n",
        "# classification_report_from_scratch(y_test, predictions_test, class_names=class_labels)\n",
        "\n",
        "\n",
        "# test_accuracy = np.mean(predictions_test == y_test)\n",
        "# print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ab60ffd-79e9-45d4-9c64-3434411664ec",
      "metadata": {
        "id": "6ab60ffd-79e9-45d4-9c64-3434411664ec"
      },
      "source": [
        "## Part 5. Model 3 - Transformer from Scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3dad4560-3d12-4d84-bb37-66cc9954db3b",
      "metadata": {
        "id": "3dad4560-3d12-4d84-bb37-66cc9954db3b"
      },
      "source": [
        "### 1. Positional Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1a3d9879-a8ad-44f2-95d7-737c6da3e36b",
      "metadata": {
        "id": "1a3d9879-a8ad-44f2-95d7-737c6da3e36b"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "class PositionalEncoding(Layer):\n",
        "    \"\"\"\n",
        "    Injects positional information into the input embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, max_seq_len, embed_dim):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.max_seq_len = max_seq_len\n",
        "\n",
        "        # Pre-calculate the positional encoding matrix\n",
        "        pe = np.zeros((max_seq_len, embed_dim))\n",
        "        position = np.arange(0, max_seq_len).reshape(-1, 1)\n",
        "        div_term = np.exp(np.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
        "\n",
        "        pe[:, 0::2] = np.sin(position * div_term)\n",
        "        pe[:, 1::2] = np.cos(position * div_term)\n",
        "\n",
        "        # Add a batch dimension for broadcasting\n",
        "        self.pe = pe.reshape(1, max_seq_len, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Adds positional encoding to the input.\n",
        "        x shape: (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # x is the output from the embedding layer\n",
        "        self.inputs = x\n",
        "        seq_len = x.shape[1]\n",
        "\n",
        "        # Add the pre-computed encodings to the input embeddings\n",
        "        return x + self.pe[:, :seq_len, :]\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"\n",
        "        Passes the gradient through, as this layer has no trainable parameters.\n",
        "        \"\"\"\n",
        "        # The gradient of the input is just the upstream gradient,\n",
        "        # as the operation is a simple addition.\n",
        "        return grad"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a7cd6e2-b36c-4940-848d-c97e5fbb9d47",
      "metadata": {
        "id": "5a7cd6e2-b36c-4940-848d-c97e5fbb9d47"
      },
      "source": [
        "### 2. Multi-Head Self Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "f0592b81-3e19-454f-a201-8f608d51f8ed",
      "metadata": {
        "id": "f0592b81-3e19-454f-a201-8f608d51f8ed"
      },
      "outputs": [],
      "source": [
        "def softmax(x):\n",
        "    \"\"\"Numerically stable softmax for the last axis.\"\"\"\n",
        "    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "    return e_x / np.sum(e_x, axis=-1, keepdims=True)\n",
        "\n",
        "class MultiHeadAttention(Layer):\n",
        "    \"\"\"\n",
        "    Multi-Head Self-Attention Layer.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads):\n",
        "        super().__init__()\n",
        "        assert embed_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads.\"\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "\n",
        "        # Xavier initialization\n",
        "        xavier_std = np.sqrt(2.0 / (embed_dim * 2))\n",
        "\n",
        "        # We can combine W_q, W_k, W_v into one matrix for efficiency\n",
        "        self.params['W_qkv'] = np.random.randn(embed_dim, embed_dim * 3) * xavier_std\n",
        "        self.params['W_o'] = np.random.randn(embed_dim, embed_dim) * xavier_std\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass for Multi-Head Attention.\n",
        "        x shape: (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, _ = x.shape\n",
        "        self.x_shape = x.shape # Store original shape\n",
        "\n",
        "        # 1. Project to Q, K, V\n",
        "        qkv = x @ self.params['W_qkv']\n",
        "\n",
        "        # 2. Reshape and split Q, K, V for each head\n",
        "        qkv = qkv.reshape(batch_size, seq_len, 3, self.num_heads, self.head_dim)\n",
        "        qkv = qkv.transpose(2, 0, 3, 1, 4) # (3, batch_size, num_heads, seq_len, head_dim)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        # 3. Scaled Dot-Product Attention\n",
        "        scores = (q @ k.transpose(0, 1, 3, 2)) / np.sqrt(self.head_dim)\n",
        "\n",
        "        # 4. Apply softmax to get attention weights\n",
        "        self.attention_weights = softmax(scores)\n",
        "\n",
        "        # 5. Apply weights to values\n",
        "        weighted_v = self.attention_weights @ v\n",
        "\n",
        "        # 6. Concatenate heads\n",
        "        # Transpose back: (batch_size, seq_len, num_heads, head_dim)\n",
        "        weighted_v = weighted_v.transpose(0, 2, 1, 3)\n",
        "        # Reshape to (batch_size, seq_len, embed_dim)\n",
        "        concat_v = weighted_v.reshape(batch_size, seq_len, self.embed_dim)\n",
        "\n",
        "        # 7. Final linear projection\n",
        "        output = concat_v @ self.params['W_o']\n",
        "\n",
        "        # Cache values for backward pass\n",
        "        self.cache = (x, q, k, v, concat_v)\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        \"\"\"\n",
        "        Backward pass for Multi-Head Attention. (This is complex!)\n",
        "        grad_output shape: (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        x, q, k, v, concat_v = self.cache\n",
        "        batch_size, seq_len, _ = self.x_shape\n",
        "\n",
        "        # 1. Gradient of the final projection\n",
        "        self.grads['W_o'] = concat_v.reshape(-1, self.embed_dim).T @ grad_output.reshape(-1, self.embed_dim)\n",
        "        grad_concat_v = grad_output @ self.params['W_o'].T\n",
        "\n",
        "        # 2. Un-concatenate heads\n",
        "        grad_weighted_v = grad_concat_v.reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        grad_weighted_v = grad_weighted_v.transpose(0, 2, 1, 3) # (batch, heads, seq_len, head_dim)\n",
        "\n",
        "        # 3. Gradient through attention_weights @ v\n",
        "        grad_attention_weights = grad_weighted_v @ v.transpose(0, 1, 3, 2)\n",
        "        grad_v = self.attention_weights.transpose(0, 1, 3, 2) @ grad_weighted_v\n",
        "\n",
        "        # 4. Gradient through softmax\n",
        "        s = self.attention_weights\n",
        "        grad_scores = s * (grad_attention_weights - np.sum(grad_attention_weights * s, axis=-1, keepdims=True))\n",
        "\n",
        "        # 5. Gradient through scaling\n",
        "        grad_scores /= np.sqrt(self.head_dim)\n",
        "\n",
        "        # 6. Gradient through q @ k.T\n",
        "        grad_q = grad_scores @ k\n",
        "        grad_k = grad_scores.transpose(0, 1, 3, 2) @ q\n",
        "\n",
        "        # 7. Combine head gradients for Q, K, V\n",
        "        # Transpose from (batch, head, seq, dim) to (3, batch, head, seq, dim)\n",
        "        grad_qkv = np.array([grad_q, grad_k, grad_v])\n",
        "        # Transpose back to (batch, seq, 3, head, dim)\n",
        "        grad_qkv = grad_qkv.transpose(1, 3, 0, 2, 4)\n",
        "        # Reshape to (batch, seq, 3 * embed_dim)\n",
        "        grad_qkv = grad_qkv.reshape(batch_size, seq_len, 3 * self.embed_dim)\n",
        "\n",
        "        # 8. Gradient of the initial projection W_qkv\n",
        "        self.grads['W_qkv'] = x.reshape(-1, self.embed_dim).T @ grad_qkv.reshape(-1, 3 * self.embed_dim)\n",
        "\n",
        "        # 9. Gradient for the input x\n",
        "        grad_x = grad_qkv @ self.params['W_qkv'].T\n",
        "\n",
        "        return grad_x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7066c839-d1f4-4af5-95ac-6a8c2213a42a",
      "metadata": {
        "id": "7066c839-d1f4-4af5-95ac-6a8c2213a42a"
      },
      "source": [
        "### 3. Position Wise Feedforward Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "c1b42fe4-b10c-4d81-abbe-d2a9187a104b",
      "metadata": {
        "id": "c1b42fe4-b10c-4d81-abbe-d2a9187a104b"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(Layer):\n",
        "    \"\"\"\n",
        "    Implements the Position-wise Feed-Forward Network.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, ffn_dim):\n",
        "        super().__init__()\n",
        "        # He initialization for ReLU\n",
        "        he_std_1 = np.sqrt(2.0 / embed_dim)\n",
        "        he_std_2 = np.sqrt(2.0 / ffn_dim)\n",
        "\n",
        "        self.params['W_1'] = np.random.randn(embed_dim, ffn_dim) * he_std_1\n",
        "        self.params['b_1'] = np.zeros(ffn_dim)\n",
        "        self.params['W_2'] = np.random.randn(ffn_dim, embed_dim) * he_std_2\n",
        "        self.params['b_2'] = np.zeros(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len, embed_dim)\n",
        "        \"\"\"\n",
        "        # First linear layer + ReLU\n",
        "        linear_1 = x @ self.params['W_1'] + self.params['b_1']\n",
        "        relu_out = np.maximum(0, linear_1) # ReLU activation\n",
        "\n",
        "        # Second linear layer\n",
        "        output = relu_out @ self.params['W_2'] + self.params['b_2']\n",
        "\n",
        "        # Cache for backprop\n",
        "        self.cache = (x, linear_1, relu_out)\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        x, linear_1, relu_out = self.cache\n",
        "\n",
        "        # Backprop through second linear layer\n",
        "        self.grads['W_2'] = relu_out.reshape(-1, relu_out.shape[-1]).T @ grad_output.reshape(-1, grad_output.shape[-1])\n",
        "        self.grads['b_2'] = np.sum(grad_output, axis=(0, 1))\n",
        "        grad_relu_out = grad_output @ self.params['W_2'].T\n",
        "\n",
        "        # Backprop through ReLU\n",
        "        grad_linear_1 = grad_relu_out * (linear_1 > 0)\n",
        "\n",
        "        # Backprop through first linear layer\n",
        "        self.grads['W_1'] = x.reshape(-1, x.shape[-1]).T @ grad_linear_1.reshape(-1, grad_linear_1.shape[-1])\n",
        "        self.grads['b_1'] = np.sum(grad_linear_1, axis=(0, 1))\n",
        "\n",
        "        # Gradient to pass to the previous layer\n",
        "        grad_x = grad_linear_1 @ self.params['W_1'].T\n",
        "        return grad_x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69c27848-69dc-44e3-954f-e18ce90dffba",
      "metadata": {
        "id": "69c27848-69dc-44e3-954f-e18ce90dffba"
      },
      "source": [
        "### 4. Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "71f25d8d-bce8-41e8-97a1-0429128edcf8",
      "metadata": {
        "id": "71f25d8d-bce8-41e8-97a1-0429128edcf8"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(Layer):\n",
        "    \"\"\"\n",
        "    Implements Layer Normalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, epsilon=1e-5):\n",
        "        super().__init__()\n",
        "        self.epsilon = epsilon\n",
        "        # Learnable parameters: gamma (scale) and beta (shift)\n",
        "        self.params['gamma'] = np.ones(embed_dim)\n",
        "        self.params['beta'] = np.zeros(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Calculate mean and variance along the last dimension (features)\n",
        "        mean = np.mean(x, axis=-1, keepdims=True)\n",
        "        var = np.var(x, axis=-1, keepdims=True)\n",
        "        std = np.sqrt(var + self.epsilon)\n",
        "\n",
        "        # Normalize x\n",
        "        x_norm = (x - mean) / std\n",
        "\n",
        "        # Apply scale and shift\n",
        "        output = self.params['gamma'] * x_norm + self.params['beta']\n",
        "\n",
        "        self.cache = (x, mean, std, x_norm)\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        x, mean, std, x_norm = self.cache\n",
        "        N, T, D = x.shape\n",
        "\n",
        "        # Gradients for learnable parameters gamma and beta\n",
        "        self.grads['gamma'] = np.sum(grad_output * x_norm, axis=(0, 1))\n",
        "        self.grads['beta'] = np.sum(grad_output, axis=(0, 1))\n",
        "\n",
        "        # Gradient for the normalized output\n",
        "        grad_x_norm = grad_output * self.params['gamma']\n",
        "\n",
        "        # Gradient for the standard deviation\n",
        "        grad_std = -np.sum(grad_x_norm * (x - mean), axis=-1, keepdims=True) / (std**2)\n",
        "\n",
        "        # Gradient for the variance\n",
        "        grad_var = 0.5 * grad_std / std\n",
        "\n",
        "        # Gradient for the mean\n",
        "        grad_mean = -np.sum(grad_x_norm / std, axis=-1, keepdims=True) - (2.0/D) * grad_var * np.sum(x - mean, axis=-1, keepdims=True)\n",
        "\n",
        "        # Gradient for the input x\n",
        "        grad_x = (grad_x_norm / std) + (2.0/D) * grad_var * (x - mean) + (1.0/D) * grad_mean\n",
        "\n",
        "        return grad_x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c7482bb-3b96-44c1-8781-5917a681d911",
      "metadata": {
        "id": "0c7482bb-3b96-44c1-8781-5917a681d911"
      },
      "source": [
        "### 5. Transformer Encoder Block"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "bafcccc5-72e4-481a-af9b-5291f30c029f",
      "metadata": {
        "id": "bafcccc5-72e4-481a-af9b-5291f30c029f"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(Layer):\n",
        "    \"\"\"\n",
        "    A single block of the Transformer Encoder.\n",
        "    \"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, ffn_dim, dropout_p=0.1):\n",
        "        # We REMOVE the super().__init__() call here.\n",
        "        # This class is a container and its params/grads are handled by the @property methods.\n",
        "\n",
        "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
        "        self.norm1 = LayerNormalization(embed_dim)\n",
        "        self.feed_forward = PositionwiseFeedForward(embed_dim, ffn_dim)\n",
        "        self.norm2 = LayerNormalization(embed_dim)\n",
        "        self.dropout1 = Dropout(dropout_p)\n",
        "        self.dropout2 = Dropout(dropout_p)\n",
        "\n",
        "        # Store sub-layers in a list to make collecting params/grads easy\n",
        "        self.sub_layers = [self.attention, self.norm1, self.feed_forward, self.norm2, self.dropout1, self.dropout2]\n",
        "\n",
        "    def forward(self, x, training=True):\n",
        "        # 1. Attention sub-layer\n",
        "        attn_output = self.attention.forward(x)\n",
        "        attn_output = self.dropout1.forward(attn_output, training=training)\n",
        "        # Residual connection and normalization\n",
        "        sublayer1_output = self.norm1.forward(x + attn_output)\n",
        "\n",
        "        # 2. Feed-forward sub-layer\n",
        "        ffn_output = self.feed_forward.forward(sublayer1_output)\n",
        "        ffn_output = self.dropout2.forward(ffn_output, training=training)\n",
        "        # Residual connection and normalization\n",
        "        output = self.norm2.forward(sublayer1_output + ffn_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def backward(self, grad_output):\n",
        "        # Backpropagate in reverse order\n",
        "        grad_sublayer2 = self.norm2.backward(grad_output)\n",
        "        grad_sublayer1_output = grad_sublayer2\n",
        "        grad_ffn_output = grad_sublayer2\n",
        "\n",
        "        grad_ffn_output = self.dropout2.backward(grad_ffn_output)\n",
        "        grad_sublayer1_output += self.feed_forward.backward(grad_ffn_output)\n",
        "\n",
        "        grad_sublayer1 = self.norm1.backward(grad_sublayer1_output)\n",
        "        grad_x = grad_sublayer1\n",
        "        grad_attn_output = grad_sublayer1\n",
        "\n",
        "        grad_attn_output = self.dropout1.backward(grad_attn_output)\n",
        "        grad_x += self.attention.backward(grad_attn_output)\n",
        "\n",
        "        return grad_x\n",
        "\n",
        "    # These properties dynamically gather params and grads from all sub-layers\n",
        "    @property\n",
        "    def params(self):\n",
        "        params = {}\n",
        "        for i, layer in enumerate(self.sub_layers):\n",
        "            # Use layer's class name for better readability\n",
        "            layer_name = f\"{layer.__class__.__name__}_{i}\"\n",
        "            for key, val in layer.params.items():\n",
        "                params[f'{layer_name}_{key}'] = val\n",
        "        return params\n",
        "\n",
        "    @property\n",
        "    def grads(self):\n",
        "        grads = {}\n",
        "        for i, layer in enumerate(self.sub_layers):\n",
        "            layer_name = f\"{layer.__class__.__name__}_{i}\"\n",
        "            for key, val in layer.grads.items():\n",
        "                grads[f'{layer_name}_{key}'] = val\n",
        "        return grads"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "178692f2-13be-40db-b6f2-0b05067de147",
      "metadata": {
        "id": "178692f2-13be-40db-b6f2-0b05067de147"
      },
      "source": [
        "### 6. Global Average Pooling Layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "1e13e687-dcc1-4cf5-aad9-13fc16c7195a",
      "metadata": {
        "id": "1e13e687-dcc1-4cf5-aad9-13fc16c7195a"
      },
      "outputs": [],
      "source": [
        "class GlobalAveragePooling1D(Layer):\n",
        "    \"\"\"\n",
        "    Performs global average pooling over the time/sequence dimension.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x shape: (batch_size, seq_len, embed_dim)\n",
        "        output shape: (batch_size, embed_dim)\n",
        "        \"\"\"\n",
        "        self.cache_input_shape = x.shape\n",
        "        return np.mean(x, axis=1)\n",
        "\n",
        "    def backward(self, grad):\n",
        "        \"\"\"\n",
        "        Distributes the gradient evenly back across the sequence length.\n",
        "        grad shape: (batch_size, embed_dim)\n",
        "        \"\"\"\n",
        "        _, seq_len, _ = self.cache_input_shape\n",
        "\n",
        "        # Expand grad to be broadcastable to the input shape\n",
        "        grad_expanded = np.expand_dims(grad, 1)\n",
        "\n",
        "        # The gradient for each time step is the upstream grad divided by the sequence length\n",
        "        grad_input = np.tile(grad_expanded, (1, seq_len, 1)) / seq_len\n",
        "\n",
        "        return grad_input"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9217eeb5-f0b8-4ff4-a6e7-f9681c0a5db3",
      "metadata": {
        "id": "9217eeb5-f0b8-4ff4-a6e7-f9681c0a5db3"
      },
      "source": [
        "### 7. Transformer Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "4e6962a4-c29e-483a-9553-d65f0557fdba",
      "metadata": {
        "id": "4e6962a4-c29e-483a-9553-d65f0557fdba"
      },
      "outputs": [],
      "source": [
        "class TransformerClassifier:\n",
        "    \"\"\"\n",
        "    A full Transformer-based text classifier.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocab_size, max_seq_len, embed_dim, num_heads, ffn_dim, num_layers, num_classes, dropout_p=0.1):\n",
        "        self.layers = []\n",
        "\n",
        "        # 1. Input layers\n",
        "        self.embedding = Embedding(vocab_size, embed_dim)\n",
        "        self.pos_encoding = PositionalEncoding(max_seq_len, embed_dim)\n",
        "        self.embed_dropout = Dropout(dropout_p)\n",
        "        self.layers.extend([self.embedding, self.pos_encoding, self.embed_dropout])\n",
        "\n",
        "        # 2. Stack of N Transformer Encoder Blocks\n",
        "        self.encoder_blocks = []\n",
        "        for _ in range(num_layers):\n",
        "            block = TransformerEncoderBlock(embed_dim, num_heads, ffn_dim, dropout_p)\n",
        "            self.encoder_blocks.append(block)\n",
        "            self.layers.append(block)\n",
        "\n",
        "        # 3. Output layers\n",
        "        self.pooling = GlobalAveragePooling1D()\n",
        "        self.dense = Dense(embed_dim, num_classes)\n",
        "        self.layers.extend([self.pooling, self.dense])\n",
        "\n",
        "    def forward(self, inputs, training=True):\n",
        "        # Input processing\n",
        "        x = self.embedding.forward(inputs)\n",
        "        x = self.pos_encoding.forward(x)\n",
        "        x = self.embed_dropout.forward(x, training=training)\n",
        "\n",
        "        # Pass through the stack of encoder blocks\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block.forward(x, training=training)\n",
        "\n",
        "        # Pooling and final classification\n",
        "        x = self.pooling.forward(x)\n",
        "        logits = self.dense.forward(x)\n",
        "        return logits\n",
        "\n",
        "    def backward(self, grad):\n",
        "        # Backpropagate in reverse order\n",
        "        grad = self.dense.backward(grad)\n",
        "        grad = self.pooling.backward(grad)\n",
        "\n",
        "        for block in reversed(self.encoder_blocks):\n",
        "            grad = block.backward(grad)\n",
        "\n",
        "        grad = self.embed_dropout.backward(grad)\n",
        "        grad = self.pos_encoding.backward(grad)\n",
        "        self.embedding.backward(grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35d56552-d7fd-46c0-acad-5388ce9635b1",
      "metadata": {
        "id": "35d56552-d7fd-46c0-acad-5388ce9635b1"
      },
      "source": [
        "### 8. Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "23506e1b-bb3b-463b-a561-6ad505073ed4",
      "metadata": {
        "id": "23506e1b-bb3b-463b-a561-6ad505073ed4"
      },
      "outputs": [],
      "source": [
        "# from tqdm.notebook import tqdm\n",
        "# import os\n",
        "\n",
        "# MAX_SEQ_LEN = 50\n",
        "\n",
        "# VOCAB_SIZE = len(vocab)\n",
        "# EMBED_DIM = 100\n",
        "# NUM_HEADS = 5          # Number of attention heads (standard choice)\n",
        "# FFN_DIM = 400         # Hidden layer size in FFN (usually 4 * embed_dim)\n",
        "# NUM_LAYERS = 4         # Number of Transformer blocks to stack (a deep model)\n",
        "# NUM_CLASSES = 5\n",
        "# DROPOUT_P = 0.1        # Standard dropout for Transformers\n",
        "\n",
        "# EPOCHS = 10            # Train for longer to allow the model to converge\n",
        "# BATCH_SIZE = 64        # Can use a larger batch size if memory allows\n",
        "# LEARNING_RATE = 0.0001 # Transformers often prefer a smaller learning rate\n",
        "\n",
        "# # --- Initialization ---\n",
        "\n",
        "# print(\"Initializing Transformer Classifier...\")\n",
        "# transformer_model = TransformerClassifier(\n",
        "#     vocab_size=VOCAB_SIZE,\n",
        "#     max_seq_len=MAX_SEQ_LEN,\n",
        "#     embed_dim=EMBED_DIM,\n",
        "#     num_heads=NUM_HEADS,\n",
        "#     ffn_dim=FFN_DIM,\n",
        "#     num_layers=NUM_LAYERS,\n",
        "#     num_classes=NUM_CLASSES,\n",
        "#     dropout_p=DROPOUT_P\n",
        "# )\n",
        "\n",
        "# loss_fn = SoftmaxCrossEntropy()\n",
        "# optimizer = Adam(\n",
        "#     transformer_model.layers,\n",
        "#     learning_rate=LEARNING_RATE,\n",
        "#     beta1=0.9,\n",
        "#     beta2=0.98,        # Values commonly used for Transformers\n",
        "#     epsilon=1e-9,      # Values commonly used for Transformers\n",
        "#     clip_norm=1.0      # Gradient norm clipping is very important\n",
        "# )\n",
        "\n",
        "# # --- Training Loop ---\n",
        "# num_batches = len(X_train) // BATCH_SIZE\n",
        "\n",
        "# for epoch in range(EPOCHS):\n",
        "#     epoch_loss = 0\n",
        "#     permutation = np.random.permutation(len(X_train))\n",
        "#     X_train_shuffled = X_train[permutation]\n",
        "#     y_train_shuffled = y_train[permutation]\n",
        "\n",
        "#     pbar = tqdm(range(num_batches), desc=f\"Epoch {epoch+1}/{EPOCHS} (transformer)\")\n",
        "\n",
        "#     for i in pbar:\n",
        "#         start = i * BATCH_SIZE\n",
        "#         end = start + BATCH_SIZE\n",
        "#         X_batch, y_batch = X_train_shuffled[start:end], y_train_shuffled[start:end]\n",
        "\n",
        "#         logits = transformer_model.forward(X_batch, training=True)\n",
        "#         loss = loss_fn.forward(logits, y_batch)\n",
        "#         epoch_loss += loss\n",
        "\n",
        "#         grad = loss_fn.backward()\n",
        "#         transformer_model.backward(grad)\n",
        "#         optimizer.step()\n",
        "\n",
        "#         pbar.set_postfix({'loss': f'{loss:.4f}'})\n",
        "\n",
        "#     print(f\"Epoch {epoch+1} Average Loss: {epoch_loss / num_batches:.4f}\")\n",
        "\n",
        "# # --- Save the trained weights ---\n",
        "# if not os.path.exists('saved_weights'):\n",
        "#     os.makedirs('saved_weights')\n",
        "\n",
        "# # --- Save and Evaluate ---\n",
        "# save_weights(transformer_model, 'saved_weights/transformer_model_weights.npz')\n",
        "\n",
        "# # --- Evaluation ---\n",
        "# def evaluate(model, X, y):\n",
        "#     logits = model.forward(X, training=False)\n",
        "#     predictions = np.argmax(logits, axis=1)\n",
        "#     accuracy = np.mean(predictions == y)\n",
        "#     return accuracy\n",
        "\n",
        "# print(\"\\n--- Transformer Model Evaluation ---\")\n",
        "# train_accuracy = evaluate(transformer_model, X_train[:500], y_train[:500])\n",
        "# test_accuracy = evaluate(transformer_model, X_test, y_test)\n",
        "# print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "# print(f\"Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed9d5064-69e8-4ca1-be05-ce55797d350c",
      "metadata": {
        "id": "ed9d5064-69e8-4ca1-be05-ce55797d350c"
      },
      "source": [
        "### 9. Classification Report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "fbe1a22b-ba9b-4deb-9128-9fadbd2784fd",
      "metadata": {
        "id": "fbe1a22b-ba9b-4deb-9128-9fadbd2784fd"
      },
      "outputs": [],
      "source": [
        "# print(\"Evaluating on the test set...\")\n",
        "# logits_test = transformer_model.forward(X_test)\n",
        "# predictions_test = np.argmax(logits_test, axis=1)\n",
        "\n",
        "\n",
        "# # Class Names\n",
        "# class_labels = ['Neutral','Positive','Extremely Negative','Negative','Extremely Positive']\n",
        "\n",
        "# # Generate and print the report\n",
        "# print(\"\\n--- Classification Report ---\")\n",
        "# classification_report_from_scratch(y_test, predictions_test, class_names=class_labels)\n",
        "\n",
        "\n",
        "# test_accuracy = np.mean(predictions_test == y_test)\n",
        "# print(f\"\\nFinal Test Accuracy: {test_accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7c4c6efa-1aa1-4e8e-97c1-65bb9171193e",
      "metadata": {
        "id": "7c4c6efa-1aa1-4e8e-97c1-65bb9171193e"
      },
      "source": [
        "## Loading Weights for Each Model from Github Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc3dc4e9-bf15-445b-8cfc-0f59cd25fc60",
      "metadata": {
        "id": "bc3dc4e9-bf15-445b-8cfc-0f59cd25fc60"
      },
      "source": [
        "### Defining Load Weight Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "c7835247-5cf4-4dcc-bfd1-f42dc6c38a49",
      "metadata": {
        "id": "c7835247-5cf4-4dcc-bfd1-f42dc6c38a49"
      },
      "outputs": [],
      "source": [
        "def load_weights_from_url(model, url):\n",
        "    \"\"\"Downloads and loads weights from a URL.\"\"\"\n",
        "    print(f\"Downloading weights from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Raise an exception for bad status codes\n",
        "    with io.BytesIO(response.content) as f:\n",
        "        data = np.load(f)\n",
        "        for i, layer in enumerate(model.layers):\n",
        "            for key in layer.params:\n",
        "                layer.params[key] = data[f'layer_{i}_{key}']\n",
        "    print(\"Weights loaded successfully from URL.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3616063-dbae-4df3-923d-9ab7fe59c30e",
      "metadata": {
        "id": "e3616063-dbae-4df3-923d-9ab7fe59c30e"
      },
      "source": [
        "### For RNN"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import io\n",
        "\n",
        "# Define Model Hyperparameters\n",
        "# These must match the parameters used to train the saved model.\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_SIZE = 256\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "# --- 2. Initialize a New, Untrained Model Instance ---\n",
        "print(\"Initializing a model structure...\")\n",
        "rnn_model_from_git = RNNClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_CLASSES)\n",
        "\n",
        "# --- 3. Define the Raw URL for the Weights File ---\n",
        "rnn_weights_url = \"https://raw.githubusercontent.com/anirudha22-stack/nlp_assignment_models/main/rnn_model_weights.npz\"\n",
        "\n",
        "# --- Evaluation ---\n",
        "def evaluate(model, X, y):\n",
        "    logits = model.forward(X)\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    return accuracy\n",
        "\n",
        "# --- 4. Load the Weights from the URL ---\n",
        "try:\n",
        "    load_weights_from_url(rnn_model_from_git, rnn_weights_url)\n",
        "\n",
        "    # --- 5. Evaluate the Model's Performance ---\n",
        "    print(\"\\nEvaluating model performance on the test set...\")\n",
        "    test_accuracy = evaluate(rnn_model_from_git, X_test, y_test)\n",
        "\n",
        "    # --- 6. Show the Final Accuracy ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"RNN Model Accuracy from GitHub: {test_accuracy:.4f}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n An error occurred: {e}\")\n",
        "    print(\"Please ensure the URL is correct and the file is accessible.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5HdTvJv3cpz",
        "outputId": "b5c2b347-6b19-4b5b-ea8c-51ee4e7ad3bf"
      },
      "id": "j5HdTvJv3cpz",
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a model structure...\n",
            "Downloading weights from https://raw.githubusercontent.com/anirudha22-stack/nlp_assignment_models/main/rnn_model_weights.npz...\n",
            "Weights loaded successfully from URL.\n",
            "\n",
            "Evaluating model performance on the test set...\n",
            "\n",
            "========================================\n",
            "RNN Model Accuracy from GitHub: 0.2509\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For LSTM"
      ],
      "metadata": {
        "id": "srmkJWZUv7W8"
      },
      "id": "srmkJWZUv7W8"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Model Hyperparameters\n",
        "# These must match the parameters used to train the saved model.\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 100\n",
        "HIDDEN_SIZE = 128\n",
        "NUM_CLASSES = 5\n",
        "\n",
        "# --- 2. Initialize a New, Untrained Model Instance ---\n",
        "print(\"Initializing a model structure...\")\n",
        "lstm_model_from_git = LSTMClassifier(VOCAB_SIZE, EMBED_DIM, HIDDEN_SIZE, NUM_CLASSES)\n",
        "\n",
        "# --- 3. Define the Raw URL for the Weights File ---\n",
        "lstm_weights_url = \"https://raw.githubusercontent.com/anirudha22-stack/nlp_assignment_models/main/lstm_model_weights.npz\"\n",
        "\n",
        "# --- Evaluation ---\n",
        "def evaluate(model, X, y):\n",
        "    logits = model.forward(X)\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    accuracy = np.mean(predictions == y)\n",
        "    return accuracy\n",
        "\n",
        "# --- 4. Load the Weights from the URL ---\n",
        "try:\n",
        "    load_weights_from_url(lstm_model_from_git, lstm_weights_url)\n",
        "\n",
        "    # --- 5. Evaluate the Model's Performance ---\n",
        "    print(\"\\nEvaluating model performance on the test set...\")\n",
        "    test_accuracy = evaluate(lstm_model_from_git, X_test, y_test)\n",
        "\n",
        "    # --- 6. Show the Final Accuracy ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"LSTM Model Accuracy from GitHub: {test_accuracy:.4f}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n An error occurred: {e}\")\n",
        "    print(\"Please ensure the URL is correct and the file is accessible.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ADmP-1O1yT5",
        "outputId": "e775ccef-4be2-4f75-b4f1-08194b9e55c2"
      },
      "id": "2ADmP-1O1yT5",
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a model structure...\n",
            "Downloading weights from https://raw.githubusercontent.com/anirudha22-stack/nlp_assignment_models/main/lstm_model_weights.npz...\n",
            "Weights loaded successfully from URL.\n",
            "\n",
            "Evaluating model performance on the test set...\n",
            "\n",
            "========================================\n",
            "LSTM Model Accuracy from GitHub: 0.7098\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### For Transformer"
      ],
      "metadata": {
        "id": "Y4jChMl4wlLJ"
      },
      "id": "Y4jChMl4wlLJ"
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import io\n",
        "\n",
        "def load_weights_from_url(model, url):\n",
        "    \"\"\"\n",
        "    Downloads and loads weights, matching the original simple saving function's keys.\n",
        "    \"\"\"\n",
        "    print(f\"Downloading weights from {url}...\")\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Will raise an error if download fails\n",
        "\n",
        "    # Load the weights from the downloaded content into memory\n",
        "    weights_from_file = np.load(io.BytesIO(response.content))\n",
        "    print(\"Downloaded file contains the following keys:\", list(weights_from_file.keys()))\n",
        "\n",
        "    print(\"\\n--- Attempting to match and load weights into model layers ---\")\n",
        "\n",
        "    # This loop MUST perfectly mirror your original save_weights function\n",
        "    # Iterate through the model's main layers with an index (i)\n",
        "    for i, layer in enumerate(model.layers):\n",
        "\n",
        "        # Iterate through the parameters within that layer\n",
        "        # For TransformerEncoderBlock, this correctly calls the @property\n",
        "        for key, param_obj in layer.params.items():\n",
        "\n",
        "            # Reconstruct the exact key name that was saved in the file\n",
        "            param_key = f'layer_{i}_{key}'\n",
        "\n",
        "            # Check if this reconstructed key exists in the loaded file\n",
        "            if param_key in weights_from_file:\n",
        "                # Sanity check: ensure the shapes match\n",
        "                assert param_obj.shape == weights_from_file[param_key].shape, \\\n",
        "                    f\"Shape mismatch for '{param_key}': Model expects {param_obj.shape}, file has {weights_from_file[param_key].shape}\"\n",
        "\n",
        "                # Assign the loaded weights to the model's parameter object\n",
        "                param_obj[:] = weights_from_file[param_key]\n",
        "                print(f\" Successfully loaded {param_key}\")\n",
        "            else:\n",
        "                print(f\" WARNING: Weight key '{param_key}' not found in the downloaded file.\")"
      ],
      "metadata": {
        "id": "hCbIFFQQ_2mp"
      },
      "id": "hCbIFFQQ_2mp",
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# --- 1. Define Model Hyperparameters ---\n",
        "# These must match the parameters used to train the saved model.\n",
        "MAX_SEQ_LEN = 50\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBED_DIM = 100\n",
        "NUM_HEADS = 5\n",
        "FFN_DIM = 400\n",
        "NUM_LAYERS = 4\n",
        "NUM_CLASSES = 5\n",
        "DROPOUT_P = 0.1\n",
        "\n",
        "# --- 2. Initialize a New, Untrained Model Instance ---\n",
        "print(\"Initializing a new model structure...\")\n",
        "transformer_model_from_git = TransformerClassifier(\n",
        "    vocab_size=VOCAB_SIZE,\n",
        "    max_seq_len=MAX_SEQ_LEN,\n",
        "    embed_dim=EMBED_DIM,\n",
        "    num_heads=NUM_HEADS,\n",
        "    ffn_dim=FFN_DIM,\n",
        "    num_layers=NUM_LAYERS,\n",
        "    num_classes=NUM_CLASSES,\n",
        "    dropout_p=DROPOUT_P\n",
        ")\n",
        "\n",
        "# --- 3. Define the Raw URL for the Weights File ---\n",
        "transformer_weights_url = \"https://raw.githubusercontent.com/anirudha22-stack/nlp_assignment_models/main/transformer_model_weights.npz\"\n",
        "\n",
        "# --- 4. Define the Batched Evaluation Function ---\n",
        "def evaluate(model, X, y, batch_size=64):\n",
        "    \"\"\"\n",
        "    Evaluates the model on the given data in mini-batches to prevent memory crashes.\n",
        "    \"\"\"\n",
        "    num_samples = len(X)\n",
        "    num_batches = (num_samples + batch_size - 1) // batch_size\n",
        "\n",
        "    all_predictions = []\n",
        "\n",
        "    print(f\"Evaluating on {num_samples} samples in batches of {batch_size}...\")\n",
        "    pbar = tqdm(range(num_batches), desc=\"Evaluation Progress\")\n",
        "\n",
        "    for i in pbar:\n",
        "        start_idx = i * batch_size\n",
        "        end_idx = start_idx + batch_size\n",
        "        X_batch = X[start_idx:end_idx]\n",
        "\n",
        "        logits = model.forward(X_batch, training=False)\n",
        "        predictions = np.argmax(logits, axis=1)\n",
        "        all_predictions.extend(predictions.tolist())\n",
        "\n",
        "    all_predictions = np.array(all_predictions)\n",
        "    accuracy = np.mean(all_predictions == y)\n",
        "    return accuracy\n",
        "\n",
        "# --- 5. Load Weights and Evaluate the Model ---\n",
        "try:\n",
        "    # --- Step 5a: Load the Weights ---\n",
        "    print(\"\\nAttempting to load weights from URL...\")\n",
        "    load_weights_from_url(transformer_model_from_git, transformer_weights_url)\n",
        "    print(\" Weights successfully loaded into the model.\")\n",
        "\n",
        "    # --- Step 5b: Evaluate the Model's Performance ---\n",
        "    print(\"\\nEvaluating model performance on the test set...\")\n",
        "    # We only need to call the evaluate function ONCE.\n",
        "    test_accuracy = evaluate(transformer_model_from_git, X_test, y_test, batch_size=64)\n",
        "\n",
        "    # --- Step 5c: Show the Final Accuracy ---\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(f\"Transformer Model Accuracy from GitHub: {test_accuracy:.4f}\")\n",
        "    print(\"=\"*40)\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n An error occurred: {e}\")\n",
        "    print(\"Please ensure the URL is correct and the weights file is accessible.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0072da251a5140cb9acfaf6615146428",
            "ee0ba418435c48a6834c9c85c1ee7d8b",
            "695083e0b58146748937f9ac8dc499d9",
            "4e3e7b6cdb5541319b5b4dbd4dddcce0",
            "bf292b9f751b434cbea082843d85927f",
            "e06482eef1e4474cb1de9741038557d2",
            "9837480426904c70a5c50f8380af082d",
            "7cd9248dbad4470b8b059a1d8959fa3e",
            "0cd8f02456d3499584109d042c92756b",
            "aa046e6d25e344129748dea595454b7a",
            "bac8bbbf532f4799a6cc2d56ffaaca43"
          ]
        },
        "id": "lLj0jWkRzD_r",
        "outputId": "0479db2b-3ce2-48df-cf6d-58ae7882f077"
      },
      "id": "lLj0jWkRzD_r",
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing a new model structure...\n",
            "\n",
            "Attempting to load weights from URL...\n",
            "Downloading weights from https://raw.githubusercontent.com/anirudha22-stack/nlp_assignment_models/main/transformer_model_weights.npz...\n",
            "Downloaded file contains the following keys: ['layer_0_W', 'layer_3_MultiHeadAttention_0_W_qkv', 'layer_3_MultiHeadAttention_0_W_o', 'layer_3_LayerNormalization_1_gamma', 'layer_3_LayerNormalization_1_beta', 'layer_3_PositionwiseFeedForward_2_W_1', 'layer_3_PositionwiseFeedForward_2_b_1', 'layer_3_PositionwiseFeedForward_2_W_2', 'layer_3_PositionwiseFeedForward_2_b_2', 'layer_3_LayerNormalization_3_gamma', 'layer_3_LayerNormalization_3_beta', 'layer_4_MultiHeadAttention_0_W_qkv', 'layer_4_MultiHeadAttention_0_W_o', 'layer_4_LayerNormalization_1_gamma', 'layer_4_LayerNormalization_1_beta', 'layer_4_PositionwiseFeedForward_2_W_1', 'layer_4_PositionwiseFeedForward_2_b_1', 'layer_4_PositionwiseFeedForward_2_W_2', 'layer_4_PositionwiseFeedForward_2_b_2', 'layer_4_LayerNormalization_3_gamma', 'layer_4_LayerNormalization_3_beta', 'layer_5_MultiHeadAttention_0_W_qkv', 'layer_5_MultiHeadAttention_0_W_o', 'layer_5_LayerNormalization_1_gamma', 'layer_5_LayerNormalization_1_beta', 'layer_5_PositionwiseFeedForward_2_W_1', 'layer_5_PositionwiseFeedForward_2_b_1', 'layer_5_PositionwiseFeedForward_2_W_2', 'layer_5_PositionwiseFeedForward_2_b_2', 'layer_5_LayerNormalization_3_gamma', 'layer_5_LayerNormalization_3_beta', 'layer_6_MultiHeadAttention_0_W_qkv', 'layer_6_MultiHeadAttention_0_W_o', 'layer_6_LayerNormalization_1_gamma', 'layer_6_LayerNormalization_1_beta', 'layer_6_PositionwiseFeedForward_2_W_1', 'layer_6_PositionwiseFeedForward_2_b_1', 'layer_6_PositionwiseFeedForward_2_W_2', 'layer_6_PositionwiseFeedForward_2_b_2', 'layer_6_LayerNormalization_3_gamma', 'layer_6_LayerNormalization_3_beta', 'layer_8_W', 'layer_8_b']\n",
            "\n",
            "--- Attempting to match and load weights into model layers ---\n",
            " Successfully loaded layer_0_W\n",
            " Successfully loaded layer_3_MultiHeadAttention_0_W_qkv\n",
            " Successfully loaded layer_3_MultiHeadAttention_0_W_o\n",
            " Successfully loaded layer_3_LayerNormalization_1_gamma\n",
            " Successfully loaded layer_3_LayerNormalization_1_beta\n",
            " Successfully loaded layer_3_PositionwiseFeedForward_2_W_1\n",
            " Successfully loaded layer_3_PositionwiseFeedForward_2_b_1\n",
            " Successfully loaded layer_3_PositionwiseFeedForward_2_W_2\n",
            " Successfully loaded layer_3_PositionwiseFeedForward_2_b_2\n",
            " Successfully loaded layer_3_LayerNormalization_3_gamma\n",
            " Successfully loaded layer_3_LayerNormalization_3_beta\n",
            " Successfully loaded layer_4_MultiHeadAttention_0_W_qkv\n",
            " Successfully loaded layer_4_MultiHeadAttention_0_W_o\n",
            " Successfully loaded layer_4_LayerNormalization_1_gamma\n",
            " Successfully loaded layer_4_LayerNormalization_1_beta\n",
            " Successfully loaded layer_4_PositionwiseFeedForward_2_W_1\n",
            " Successfully loaded layer_4_PositionwiseFeedForward_2_b_1\n",
            " Successfully loaded layer_4_PositionwiseFeedForward_2_W_2\n",
            " Successfully loaded layer_4_PositionwiseFeedForward_2_b_2\n",
            " Successfully loaded layer_4_LayerNormalization_3_gamma\n",
            " Successfully loaded layer_4_LayerNormalization_3_beta\n",
            " Successfully loaded layer_5_MultiHeadAttention_0_W_qkv\n",
            " Successfully loaded layer_5_MultiHeadAttention_0_W_o\n",
            " Successfully loaded layer_5_LayerNormalization_1_gamma\n",
            " Successfully loaded layer_5_LayerNormalization_1_beta\n",
            " Successfully loaded layer_5_PositionwiseFeedForward_2_W_1\n",
            " Successfully loaded layer_5_PositionwiseFeedForward_2_b_1\n",
            " Successfully loaded layer_5_PositionwiseFeedForward_2_W_2\n",
            " Successfully loaded layer_5_PositionwiseFeedForward_2_b_2\n",
            " Successfully loaded layer_5_LayerNormalization_3_gamma\n",
            " Successfully loaded layer_5_LayerNormalization_3_beta\n",
            " Successfully loaded layer_6_MultiHeadAttention_0_W_qkv\n",
            " Successfully loaded layer_6_MultiHeadAttention_0_W_o\n",
            " Successfully loaded layer_6_LayerNormalization_1_gamma\n",
            " Successfully loaded layer_6_LayerNormalization_1_beta\n",
            " Successfully loaded layer_6_PositionwiseFeedForward_2_W_1\n",
            " Successfully loaded layer_6_PositionwiseFeedForward_2_b_1\n",
            " Successfully loaded layer_6_PositionwiseFeedForward_2_W_2\n",
            " Successfully loaded layer_6_PositionwiseFeedForward_2_b_2\n",
            " Successfully loaded layer_6_LayerNormalization_3_gamma\n",
            " Successfully loaded layer_6_LayerNormalization_3_beta\n",
            " Successfully loaded layer_8_W\n",
            " Successfully loaded layer_8_b\n",
            " Weights successfully loaded into the model.\n",
            "\n",
            "Evaluating model performance on the test set...\n",
            "Evaluating on 3798 samples in batches of 64...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluation Progress:   0%|          | 0/60 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0072da251a5140cb9acfaf6615146428"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================================\n",
            "Transformer Model Accuracy from GitHub: 0.7227\n",
            "========================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "b5ZFJ1oT-rEF"
      },
      "id": "b5ZFJ1oT-rEF",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [conda env:base] *",
      "language": "python",
      "name": "conda-base-py"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0072da251a5140cb9acfaf6615146428": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ee0ba418435c48a6834c9c85c1ee7d8b",
              "IPY_MODEL_695083e0b58146748937f9ac8dc499d9",
              "IPY_MODEL_4e3e7b6cdb5541319b5b4dbd4dddcce0"
            ],
            "layout": "IPY_MODEL_bf292b9f751b434cbea082843d85927f"
          }
        },
        "ee0ba418435c48a6834c9c85c1ee7d8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e06482eef1e4474cb1de9741038557d2",
            "placeholder": "",
            "style": "IPY_MODEL_9837480426904c70a5c50f8380af082d",
            "value": "EvaluationProgress:100%"
          }
        },
        "695083e0b58146748937f9ac8dc499d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cd9248dbad4470b8b059a1d8959fa3e",
            "max": 60,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0cd8f02456d3499584109d042c92756b",
            "value": 60
          }
        },
        "4e3e7b6cdb5541319b5b4dbd4dddcce0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_aa046e6d25e344129748dea595454b7a",
            "placeholder": "",
            "style": "IPY_MODEL_bac8bbbf532f4799a6cc2d56ffaaca43",
            "value": "60/60[00:26&lt;00:00,1.32it/s]"
          }
        },
        "bf292b9f751b434cbea082843d85927f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e06482eef1e4474cb1de9741038557d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9837480426904c70a5c50f8380af082d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cd9248dbad4470b8b059a1d8959fa3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cd8f02456d3499584109d042c92756b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa046e6d25e344129748dea595454b7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bac8bbbf532f4799a6cc2d56ffaaca43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}